--- 
title: "Sports Data Analysis and Visualization for Journalists and Other Storytellers"
author: "Matt Waite"
date: "`r Sys.Date()`"
bibliography:
- book.bib
- packages.bib
description: This book is the companion to the University of Nebraska-Lincoln's SPMC
  350 course in the College of Journalism and Mass Communications.
documentclass: book
link-citations: yes
site: bookdown::bookdown_site
biblio-style: apalike
---

# Throwing cold water on hot takes

The 2018 season started out disasterously for the Nebraska Cornhuskers. The first game against a probably overmatched opponent? Called on account of an epic thunderstorm that plowed right over Memorial Stadium. The next game? Loss. The one following? Loss. The next four? All losses, after the fanbase was whipped into a hopeful frenzy by the hiring of Scott Frost, national title winning quarterback turned hot young coach come back home to save a mythical football program from the mediocrity it found itself mired in. 

All that excitement lay in tatters.

On sports talk radio, on the sports pages and across social media and cafe conversations, one topic kept coming up again and again to explain why the team was struggling: Penalties. The team was just committing too many of them. In fact, six games and no wins into the season, they were dead last in the FBS penalty yards.

Worse yet for this line of reasoning? Nebraska won game 7, against Minnesota, committing only six penalites for 43 yards, just about half their average over the season. Then they won game 8 against FCS patsy Bethune Cookman, committing only five penalties for 35 yards. That's a whopping 75 yards less than when they were losing. See? Cut the penalties, win games screamed the radio show callers. 

The problem? It's not true. Penalties might matter for a single drive. They may even throw a single game. But if you look at every top-level college football team since 2009, the number of penalty yards the team racks up means absolutely nothing to the total number of points they score. There's no relationship between them. Penalty yards have no discernable influence on points beyond just random noise. 

Put this another way: If you were Scott Frost, and a major college football program was paying you $5 million a year to make your team better, what should you focus on in practice? If you had growled at some press conference that you're going to work on penalties in practice until your team stops committing them, the results you'd get from all that wasted practice time would be impossible to separate from just random chance. You very well may reduce your penalty yards and still lose. 

How do I know this? Simple statistics. 

That's one of the three pillars of this book: Simple stats. The three pillars are:

1. Simple, easy to understand statistics ... 
2. ... extracted using simple code ...
3.  ... visualized simply to reveal new and interesting things in sports. 

Do you need to be a math whiz to read this book? No. I'm not one either. What we're going to look at is pretty basic, but that's also why it's so powerful. 

Do you need to be a computer science major to write code? Nope. I'm not one of those either. But anyone can think logically, and write simple code that is repeatable and replicable. 

Do you need to be an artist to create compelling visuals? I think you see where this is going. No. I can barely draw stick figures, but I've been paid to make graphics in my career. With a little graphic design know how, you can create publication worthy graphics with code. 

## Requirements and Conventions

This book is all in the R statistical language. To follow along, you'll do the following:

1. Install the R language on your computer. Go to the [R Project website](https://www.r-project.org/), click download R and select a mirror closest to your location. Then download the version for your computer. 
2. Install [R Studio Desktop](https://www.rstudio.com/products/rstudio/#Desktop). The free version is great. 

Going forward, you'll see passages like this:


```{r eval=FALSE}
install.packages("tidyverse")
```

Don't do it now, but that is code that you'll need to run in your R Studio. When you see that, you'll know what to do.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

# The very basics {#Basics}

R is a programming language, one specifically geared toward statistical analysis. Like all programming languages, it has certain built in functions and you can interact with it in multiple ways. The first, and most basic, is the console. 

```{r, echo=FALSE}
knitr::include_graphics(rep("images/verybasics1.png"))
```

Think of the console like talking directly to R. It's direct, but it has some drawbacks and some quirks we'll get into later. For now, try typing this into the console and hit enter: 

```{r}
2+2
```

Congrats, you've run some code. It's not very complex, and you knew the answer before hand, but you get the idea. We can compute things. We can also store things. In programming languages, these are called variables or objects. We can assign things to variables using `<-`. And then we can do things with them. Try this in your console. 

```{r}
number <- 2

number * number
```

We can have as many variables as we can name. We can even reuse them (but be careful you know you're doing that or you'll introduce errors). Try this in your console.

```{r}
firstnumber <- 1
secondnumber <-2 

(firstnumber + secondnumber) * secondnumber

```

We can store anything in a variable. A whole table. An array of numbers. A single word. A whole book. All the books of the 18th century. They're really powerful. We'll exlore them at length. 

## Adding libraries, part 1

The real strength of any given programming language is the external libraries that power it. The base language can do a lot, but it's the external libraries that solve many specific problems -- even making the base language easier to use. 

For this class, we're going to need several external libraries. 

The first library we're going to use is called Swirl. So in the console, type `install.packages('swirl')` and hit enter. That installs swirl.

Now, to use the library, type `library(swirl)` and hit enter. That loads swirl. Then type `swirl()` and hit enter. Now you're running swirl. Follow the directions on the screen. When you are asked, you want to install course 1 R Programming: The basics of programming in R. Then, when asked, you want to do option 1, R Programming, in that course. 

When you are finished with the course -- it will take just a few minutes -- type 0 to exit (it will not be clear that's what you do when you are done). 

## Adding libraries, part 2

We'll mostly use two libraries for analysis -- `dplyr` and `ggplot2`. To get them, and several other useful libraries, we can install a single collection of libraries called the tidyverse. Type this into your console: `install.packages('tidyverse')`

**NOTE**: This is a pattern. You should always install libraries in the console. 

Then, to help us with learning and replication, we're going to use R Notebooks. So we need to install that library. Type this into your console: `install.packages('rmarkdown')`

## Notebooks

For the rest of the class, we're going to be working in notebooks. In notebooks, you will both run your code and explain each step, much as I am doing here. 

To start a notebook, you click on the green plus in the top left corner and go down to R Notebook. Do that now. 

```{r, echo=FALSE}
knitr::include_graphics(rep("images/verybasics2.png"))
```

You will see that the notebook adds a lot of text for you. It tells you how to work in notebooks -- and you should read it. The most important parts are these: 

To add text, simply type. To add code you can click on the *Insert* button on the toolbar or by pressing *Cmd+Option+I* on Mac or *Ctl+Option+I* on Windows.

Highlight all that text and delete it. You should have a blank document. This document is called a R Markdown file -- it's a special form of text, one that you can style, and one you can include R in the middle of it. Markdown is a simple markup format that you can use to create documents. So first thigns first, let's give our notebook a big headline. Add this:

`# My awesome notebook` 

Now, under that, without any markup, just type This is my awesome notebook.

Under that, you can make text bold by writing `It is **really** awesome`.

If you want it italics, just do this on the next line: `No, it's _really_ awesome. I swear.`

To see what it looks like without the markup, click the Preview or Knit button in the toolbar. That will turn you notebook into a webpage, with the formatting included.

Throughout this book, we're going to use this markdown to explain what we are doing and, more importantly, why we are doing it. Explaining your thinking is a vital part of understanding what you are doing. 

That explaination, plus the code, is the real power of notebooks. To add a block of code, follow the instructions from above: click on the *Insert* button on the toolbar or by pressing *Cmd+Option+I* on Mac or *Ctl+Option+I* on Windows.

In that window, use some of the code from above and add two numbers together. To see it run, click the green triangle on the right. That runs the chunk. You should see the answer to your addition problem. 

And that, just that, is the foundation you need to start this book.


<!--chapter:end:01-intro.Rmd-->

# Data, structures and types

Data are everywhere (and data is plural of datum, thus the use of are in that statement). It surrounds you. Every time you use your phone, you are creating data. Lots of it. Your online life. Any time you buy something. It's everywhere. Sports, like life, is no different. Sports is drowning in data, and more comes along all the time.

In sports, and in this class, we'll be dealing largely with two kinds of data: event level data and summary data. It's not hard to envision event level data in sports. A pitch in baseball. A hit. A play in football. A pass in soccer. They are the events that make up the game. Combine them together -- summarize them -- and you'll have some notion of how the game went. What we usually see is summary data -- who wants to scroll through 50 pitches to find out a player went 2-3 with a double and an RBI? Who wants to scroll through hundreds of pitches to figure out the Rays beat the Yankees? 

To start with, we need to understand the shape of data. 

> EXERCISE: Try scoring a child's board game. For example, Chutes and Ladders. If you were placed in charge of analytics for the World Series of Chutes and Ladders, what is your event level data? What summary data do you keep? If you've got the game, try it. 

## Rows and columns

Data, oversimplifying it a bit, is information organized. Generally speaking, it's organized into rows and columns. Rows, generally, are individual elements. A team. A player. A game. Columns, generally, are components of the data, sometimes called variables. So if each row is a player, the first column might be their name. The second is their position. The third is their batting average. And so on. 

```{r, echo=FALSE}
knitr::include_graphics(rep("images/data1.png"))
```

One of the critical components of data analysis, especially for beginners, is having a mental picture of your data. What does each row mean? What does each column in each row signify? How many rows do you have? How many columns? 

## A simple way to get data

One good thing about sports is that there's lots of interest in it. And that means there's outlets that put sports data on the internet. Now I'm going to show you a trick to getting it easily. 

The site sports-reference.com takes NCAA (and other league) stats and puts them online. For instance, [here's their page on Nebraska basketball's game logs](https://www.sports-reference.com/cbb/schools/nebraska/2019-gamelogs.html), which you should open now.

Now, in a new tab, log into Google Docs/Drive and open a new spreadsheet. In the first cell of the first row, copy and paste this formula in:

```
=IMPORTHTML("https://www.sports-reference.com/cbb/schools/nebraska/2019-gamelogs.html", "table", 1)
```

If it worked right, you've got the data from that page in a spreadsheet. 

## Cleaning the data

The first thing we need to do is recognize that we don't have data, really. We have the results of a formula. You can tell by putting your cursor on that field, where you'll see the formula again. This is where you'd look:

```{r, echo=FALSE}
knitr::include_graphics(rep("images/clean1.png"))
```

The solution is easy:

Edit > Select All or type command/control A
Edit > Copy or type command/control c
Edit > Paste Special > Values Only or type command/control shift v

You can verify that it worked by looking in that same row 1 column A, where you'll see the formula is gone.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/clean2.png"))
```

Now you have data, but your headers are all wrong. You want your headers to be one line -- not two, like they have. And the header names repeat -- first for our team, then for theirs. So you have to change each header name to be UsORB or TeamORB and OpponentORB instead of just ORB.

After you've done that, note we have repeating headers. There's two ways to deal with that -- you could just hightlight it and go up to Edit > Delete Rows XX-XX depending on what rows you highlighted. That's the easy way with our data. 

But what if you had hundreds of repeating headers like that? Deleting them would take a long time. 

You can use sorting to get rid of anything that's not data. So click on Data > Sort Range. You'll want to check the "Data has header row" field. Then hit Sort.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/clean3.png"))
```

Now all you need to do is search through the data for where your junk data -- extra headers, blanks, etc. -- got sorted and delete it. After you've done that, you can export it for use in R. Go to File > Download as > Comma Separated Values. Remember to put it in the same directory as your R Notebook file so you can import the data easily.

<!--chapter:end:02-data.Rmd-->

# Aggregates

R is a statistical programming language that is purpose built for data analysis.

Base R does a lot, but there are a mountain of external libraries that do things to make R better/easier/more fully featured. We already installed the tidyverse -- or you should have if you followed the instructions for the last assignment -- which isn't exactly a library, but a collection of libraries. Together, they make up the tidyverse. Individually, they are extraordinarily useful for what they do. We can load them all at once using the tidyverse name, or we can load them individually. Let's start with individually. 

The two libraries we are going to need for this assignement are `readr` and `dplyr`. The library `readr` reads different types of data in. For this assignment, we're going to read in csv data or Comma Separated Values data. That's data that has a comma between each column of data. 

Then we're going to use `dplyr` to analyze it. 

To use a library, you need to import it. Good practice -- one I'm going to insist on -- is that you put all your library steps at the top of your notebooks. 

That code looks like this:

```{r}
library(readr)
```

To load them both, you need to run that code twice:

```{r}
library(readr)
library(dplyr)
```

You can keep doing that for as many libraries as you need. I've seen notebooks with 10 or more library imports.

## Basic data analysis: Group By and Count

The first thing we need to do is get some data to work with. We do that by reading it in. In our case, we're going to read data from a csv file -- a comma-separated values file.

The CSV file we're going to read from is a [Nebraska Game and Parks Commission dataset](https://www.dropbox.com/s/2l282ril6h78w8e/mountainlions.csv?dl=0) of confirmed mountain lion sightings in Nebraska. There are, on occasion, fierce debates about mountain lions and if they should be hunted in Nebraska. This dataset can tell us some interesting things about that debate. 

So step 1 is to import the data. The code looks *something* like this, but hold off copying it just yet:

`mountainlions <- read_csv("~/Documents/Data/mountainlions.csv")`

Let's unpack that.

The first part -- mountainlions -- is the name of your variable. A variable is just a name of a thing. In this case, our variable is a data frame, which is R's way of storing data. We can call this whatever we want. I always want to name data frames after what is in it. In this case, we're going to import a dataset of mountain lion sightings from the Nebraska Game and Parks Commission. Variable names, by convention are one word all lower case. You can end a variable with a number, but you can't start one with a number.

The <- bit is the variable assignment operator. It's how we know we're assigning something to a word. Think of the arrow as saying "Take everything on the right of this arrow and stuff it into the thing on the left." So we're creating an empty vessel called mountainlions and stuffing all this data into it. 

The `read_csv` bits are pretty obvious, except for one thing. What happens in the quote marks is the path to the data. In there, I have to tell R where it will find the data. The easiest thing to do, if you are confused about how to find your data, is to put your data in the same folder as as your notebook (you'll have to save that notebook first). If you do that, then you just need to put the name of the file in there (mountainlions.csv). In my case, I've got a folder called Documents in my home directory (that's the `~` part), and in there is a folder called Data that has the file called mountainlions.csv in it. Some people -- insane people -- leave the data in their downloads folder. The data path then would be `~/Downloads/nameofthedatafilehere.csv` on PC or Mac.

**What you put in there will be different from mine**. So your first task is to import the data.

```{r}
mountainlions <- read_csv("https://raw.githubusercontent.com/mattwaite/SPMC350-Sports-Data-Analysis-And-Visualization/master/Data/mountainlions.csv")
```

Now we can inspect the data we imported. What does it look like? To do that, we use `head(mountainlions)` to show the headers and the first six rows of data. If we wanted to see them all, we could just simply enter `mountainlions` and run it.

To get the number of records in our dataset, we run `nrow(mountainlions)`

```{r}
head(mountainlions)
nrow(mountainlions)
```

So what if we wanted to know how many mountain lion sightings there were in each county? To do that by hand, we'd have to take each of the 393 records and sort them into a pile. We'd put them in groups and then count them.

`dplyr` has a group by function in it that does just this. A massive amount of data analysis involves grouping like things together at some point. So it's a good place to start.

So to do this, we'll take our dataset and we'll introduce a new operator: %>%. The best way to read that operator, in my opinion, is to interpret that as "and then do this." Here's the code:

```{r}
mountainlions %>%
  group_by(COUNTY) %>%
  summarise(
    total = n()
  )
```

So let's walk through that. We start with our dataset -- `mountainlions` -- and then we tell it to group the data by a given field in the data. In this case, we wanted to group together all the counties, signified by the field name COUNTY, which you could get from looking at `head(mountainlions)`. After we group the data, we need to count them up. In dplyr, we use `summarize` [which can do more than just count things](http://dplyr.tidyverse.org/reference/summarise.html). Inside the parentheses in summarize, we set up the summaries we want. In this case, we just want a count of the counties: `count = n(),` says create a new field, called `total` and set it equal to `n()`, which might look weird, but it's common in stats. The number of things in a dataset? Statisticians call in n. There are n number of incidents in this dataset. So `n()` is a function that counts the number of things there are. 

And when we run that, we get a list of counties with a count next to them. But it's not in any order. So we'll add another And Then Do This %>% and use `arrange`. Arrange does what you think it does -- it arranges data in order. By default, it's in ascending order -- smallest to largest. But if we want to know the county with the most mountain lion sightings, we need to sort it in descending order. That looks like this:

```{r}
mountainlions %>%
  group_by(COUNTY) %>%
  summarise(
    count = n()
  ) %>% arrange(desc(count))
```

We can, if we want, group by more than one thing. So how are these sightings being confirmed? To do that, we can group by County and "Cofirm Type", which is how the state misspelled Confirm. But note something in this example below:

```{r}
mountainlions %>%
  group_by(COUNTY, `Cofirm Type`) %>%
  summarise(
    count = n()
  ) %>% arrange(desc(count))
```

See it? When you have a field name that has two words, `readr` wraps it in backticks, which is next to the 1 key on your keyboard. You can figure out which fields have backticks around it by looking at the output of `readr`. Pay attention to that, because it's coming up again in the next section and will be a part of your homework.

## Other aggregates: Mean and median

In the last example, we grouped some data together and counted it up, but there's so much more you can do. You can do multiple measures in a single step as well.

Let's look at some salary data from UNL.
```{r}
salaries <- read_csv("https://raw.githubusercontent.com/mattwaite/SPMC350-Sports-Data-Analysis-And-Visualization/master/Data/nusalaries1819.csv")
head(salaries)
```

In summarize, we can calculate any number of measures. Here, we'll use R's built in mean and median functions to calculate ... well, you get the idea.

```{r}
salaries %>%
  summarise(
    count = n(),
    mean_salary = mean(`Budgeted Annual Salary`),
    median_salary = median(`Budgeted Annual Salary`)
  )
```
So there's 13,039 employees in the database, spread across four campuses plus the system office. The mean or average salary is about \$62,000, but the median salary is slightly more than \$51,000.

Why? 

Let's let sort help us. 
```{r}
salaries %>% arrange(desc(`Budgeted Annual Salary`))
```

Oh, right. In this dataset, the university pays a football coach $5 million. Extremes influence averages, not medians, and now you have your answer.  

So when choosing a measure of the middle, you have to ask yourself -- could I have extremes? Because a median won't be sensitive to extremes. It will be the point at which half the numbers are above and half are below. The average or mean will be a measure of the middle, but if you have a bunch of low paid people and then one football coach, the average will be wildly skewed. Here, because there's so few highly paid football coaches compared to people who make a normal salary, the number is only slightly skewed in the grand scheme, but skewed nonetheless. 

<!--chapter:end:03-aggregates.Rmd-->

# Mutating data

One of the most common data analysis techniques is to look at change over time. The most common way of comparing change over time is through percent change. The math behind calculating percent change is very simple, and you should know it off the top of your head. The easy way to remember it is:

`(new - old) / old` 

Or new minus old divided by old. Your new number minus the old number, the result of which is divided by the old number. To do that in R, we can use `dplyr` and `mutate` to calculate new metrics in a new field using existing fields of data. 

So first we'll import the tidyverse so we can read in our data and begin to work with it.

```{r}
library(tidyverse)
```

Now we'll import a common and [simple dataset of total attendance](https://unl.box.com/s/hvxmnxhr41x4ikgt3vk38aczcbrf97pn) at NCAA football games over the last few seasons. 

```{r}
attendance <- read_csv('https://raw.githubusercontent.com/mattwaite/sportsdatabook/Master/data/attendance.csv')

head(attendance)
```
The code to calculate percent change is pretty simple. Remember, with `summarize`, we used `n()` to count things. With `mutate`, we use very similar syntax to calculate a new value using other values in our dataset. So in this case, we're trying to do (new-old)/old, but we're doing it with fields. If we look at what we got when we did `head`, you'll see there's \`2018\` as the new data, and we'll use \`2017\` as the old data. So we're looking at one year. Then, to help us, we'll use arrange again to sort it, so we get the fastest growing school over one year. 

```{r}
attendance %>% mutate(
  change = (`2018` - `2017`)/`2017`
) 
```
What do we see right away? Do those numbers look like we expect them to? No. They're a decimal expressed as a percentage. So let's fix that by multiplying by 100. 

```{r}
attendance %>% mutate(
  change = ((`2018` - `2017`)/`2017`)*100
) 
```
Now, does this ordering do anything for us? No. Let's fix that with arrange. 

```{r}
attendance %>% mutate(
  change = ((`2018` - `2017`)/`2017`)*100
) %>% arrange(desc(change))
```

So who had the most growth last year from the year before? Something going on at Georgia Southern. 

## A more complex example

There's metric in basketball that's easy to understand -- shooting percentage. It's the number of shots made divided by the number of shots attempted. Simple, right? Except it's a little too simple. Because what about three point shooters? They tend to be more vailable because the three point shot is worth more. What about players who get to the line? In shooting percentage, free throws are nowhere to be found. 

Basketball nerds, because of these weaknesses, have created a new metric called [True Shooting Percentage](https://en.wikipedia.org/wiki/True_shooting_percentage). True shooting percentage takes into account all aspects of a players shooting to determine who the real shooters are. 

Using `dplyr` and `mutate`, we can calculate true shooting percentage. So let's look at a new dataset, one of every college basketball player's season stats in 2018-19 season. It's a dataset of 5,386 players, and we've got 59 variables -- one of them is True Shooting Percentage, but we're going to ignore that. 

```{r}
players <- read_csv("https://raw.githubusercontent.com/mattwaite/sportsdatabook/Master/data/players19.csv")
```

The basic true shooting percentage formula is `(Points / (2*(FieldGoalAttempts + (.44 * FreeThrowAttempts)))) * 100`. Let's talk that through. Points divided by a lot. It's really field goal attempts plus 44 percent of the free throw attempts. Why? Because that's about what a free throw is worth, compared to other ways to score. After adding those things together, you double it. And after you divide points by that number, you multiply the whole lot by 100. 

In our data, we need to be able to find the fields so we can complete the formula. To do that, one way is to use the Environment tab in R Studio. In the Environment tab is a listing of all the data you've imported, and if you click the triangle next to it, it'll list all the field names, giving you a bit of information about each one. 

```{r, echo=FALSE}
knitr::include_graphics(rep("images/environment.png"))
```

So what does True Shooting Percentage look like in code? 

Let's think about this differently. Who had the best true shooting season last year? 

```{r}
players %>%
  mutate(trueshooting = (PTS/(2*(FGA + (.44*FTA))))*100) %>%
  arrange(desc(trueshooting))
```

You'll be forgiven if you did not hear about Texas Longhorns shooting sensation Drayton Whiteside. He played in six games, took one shot and actually hit it. It happened to be a three pointer, which is one more three pointer than I've hit in college basketball. So props to him. Does that mean he had the best true shooting season in college basketball last year? Not hardly. 

We'll talk about how to narrow the pile and filter out data in the next chapter.

<!--chapter:end:04-mutating.Rmd-->

# Filters and selections

More often than not, we have more data than we want. Sometimes we need to be rid of that data. In `dplyr`, there's two ways to go about this: filtering and selecting.

**Filtering creates a subset of the data based on criteria**. All records where the count is greater than 10. All records that match "Nebraska". Something like that. 

**Selecting simply returns only the fields named**. So if you only want to see School and Attendance, you select those fields. When you look at your data again, you'll have two columns. If you try to use one of your columns that you had before you used select, you'll get an error.  

Let's work with our football attendance data to show some examples.

```{r}
library(tidyverse)
```

```{r}
attendance <- read_csv('https://raw.githubusercontent.com/mattwaite/sportsdatabook/Master/data/attendance.csv')
```

So, first things first, let's say we don't care about all this Air Force, Akron, Alabama crap and just want to see Dear Old Nebraska U. We do that with `filter` and then we pass it a condition. 

Before we do that, a note about conditions. Most of the conditional operators you'll understand -- greater than and less than are > and <. The tough one to remember is equal to. In conditional statements, equal to is == not =. If you haven't noticed, = is a variable assignment operator, not a conditional statement. So equal is == and NOT equal is !=. 

So if you want to see Institutions equal to Nebraska, you do this:

```{r}
attendance %>% filter(Institution == "Nebraska")
```

Or if we want to see schools that had more than half a million people buy tickets to a football game in a season, we do the following. NOTE THE BACKTICKS. 

```{r}
attendance %>% filter(`2018` >= 500000)
```

But what if we want to see all of the Power Five conferences? We *could* use conditional logic in our filter. The conditional logic operators are `|` for OR and `&` for AND. NOTE: AND means all conditions have to be met. OR means any of the conditions work. So be careful about boolean logic. 

```{r}
attendance %>% filter(Conference == "Big 10" | Conference == "SEC" | Conference == "Pac-12" | Conference == "ACC" | Conference == "Big 12")
```
But that's a lot of repetitive code. And a lot of typing. And typing is the devil. So what if we could create a list and pass it into the filter? It's pretty simple.

We can create a new variable -- remember variables can represent just about anything -- and create a list. To do that we use the `c` operator, which stands for concatenate. That just means take all the stuff in the parenthesis after the c and bunch it into a list. 

Note here: text is in quotes. If they were numbers, we wouldn't need the quotes. 

```{r}
powerfive <- c("SEC", "Big Ten", "Pac-12", "Big 12", "ACC")
```

Now with a list, we can use the %in% operator. It does what you think it does -- it gives you data that matches things IN the list you give it. 

```{r}
attendance %>% filter(Conference %in% powerfive)
```

## Selecting data to make it easier to read

So now we have our Power Five list. What if we just wanted to see attendance from the most recent season and ignore all the rest? Select to the rescue. 

```{r}
attendance %>% filter(Conference %in% powerfive) %>% select(Institution, Conference, `2018`)
```

If you have truly massive data, Select has tools to help you select fields that start_with the same things or ends with a certain word. [The documentation will guide you](https://dplyr.tidyverse.org/reference/select.html) if you need those someday. For 90 plus percent of what we do, just naming the fields will be sufficient. 

## Using conditional filters to set limits

Let's return to the blistering season of Drayton Whiteside. How can we set limits in something like a question of who had the best season? Let's get our Drayton Whiteside data from the previous chapter back up.

```{r}
players <- read_csv("https://raw.githubusercontent.com/mattwaite/sportsdatabook/Master/data/players19.csv")

players %>%
  mutate(trueshooting = (PTS/(2*(FGA + (.44*FTA))))*100) %>%
  arrange(desc(trueshooting))
```

In most contests like the batting title in Major League Baseball, there's a minimum number of X to qualify. In baseball, it's at bats. In basketball, it attempts. So let's set a floor and see how it changes. What if we said you had to have attempted 100 shots in a season? The top shooters in college basketball attempt 1000 shots in a season. So 100 is not that much. Let's try it and see.

```{r}
players %>%
  mutate(trueshooting = (PTS/(2*(FGA + (.44*FTA))))*100) %>%
  arrange(desc(trueshooting)) %>%
  filter(FGA > 100)
```

Now you get UALR Sophomore Kris Bankston, who played in 31 games and was on the floor for 670 minutes. So he played regularly. But in that time, he only attempted 134 shots, and made 81 percent of them. In other words, when he shot, he scored. He just rarely shot. 

So is 100 attempts our level? Here's the truth -- there's not really an answer here. We're picking a cutoff. If you can cite a reason for it and defend it, then it probably works. 

## Top list

One last little dplyr trick that's nice to have in the toolbox is a shortcut for selecting only the top values for your dataset. Want to make a Top 10 List? Or Top 25? Or Top Whatever You Want? It's easy. 

So what are the top 10 Power Five schools by season attendance. All we're doing here is chaining commands together with what we've already got. We're *filtering* by our list of Power Five conferences, we're *selecting* the three fields we need, now we're going to *arrange* it by total attendance and then we'll introduce the new function: `top_n`. The `top_n` function just takes a number. So we want a top 10 list? We do it like this: 

```{r}
attendance %>% filter(Conference %in% powerfive) %>% select(Institution, Conference, `2018`) %>% arrange(desc(`2018`)) %>% top_n(10)
```
That's all there is to it. Just remember -- for it to work correctly, you need to sort your data BEFORE you run top_n. Otherwise, you're just getting the first 10 values in the list. The function doesn't know what field you want the top values of. You have to do it. 

<!--chapter:end:05-filters.Rmd-->

# Transforming data

Sometimes long data needs to be wide, and sometimes wide data needs to be long. I'll explain.

You are soon going to discover that long before you can visualize data, you need to have it in a form that the visualization library can deal with. One of the ways that isn't immediately obvious is how your data is cast. Most of the data you will encounter will be wide -- each row will represent a single entity with multiple measures for that entity. So think of states. Your row of your dataset could have population, average life expectancy and other demographic data. 

But what if your visualization library needs one row for each measure? That's where recasting your data comes in. We can use a library called `tidyr` to `gather` or `spread` the data, depending on what we need.

We'll use a dataset of college basketball games. First we need some libraries. 

```{r}
library(tidyverse)
```

Now we'll grab the data. 

```{r}
logs <- read_csv('https://raw.githubusercontent.com/mattwaite/sportsdatabook/Master/data/logs19.csv')
```

Last season, the Nebraska basketball team came out of the gates on fire. Their first 10 games were blistering. Their last 10? Not so much. But how can we compare a team's first 10 games with their last 10 games? If you look, each game in the dataset is numbered, so getting the first 10 wouldn't be hard. But what about the last five, when every team plays a different number of games? 

To find that, we need to add some data to our table. And that is the maxiumum number of games a team played. So we'll create a new dataframe where we'll group our teams by team name and get the max game number for each team. Then, we'll use something called a join, where we'll connect the max games to the logs data using the common team name to connect them. 


```{r}
max <- logs %>% group_by(Team) %>% summarise(max_games = max(Game))
```

```{r}
logs <- logs %>% left_join(max)
```

Now let's just get Nebraska. 

```{r}
nebraska <- logs %>% filter(Team == "Nebraska Cornhuskers")

head(nebraska)
```

What we have here is long data. One row, one game. But what if we wanted to calculate the percent change in two variables? Say we wanted to see what the difference in shooting percentage has been between the first 10 games and the last ten Or the percent change in that? To get that, we'd need two fields next to each other so we could mutate our dataframe to calculate that, right? You'll see the problem we have right away. 

```{r}
nebraska %>% 
  mutate(grouping = case_when(
    Game <=10 ~ "First 10",
    Game >= (max_games - 10) ~ "Last 10")
    ) %>%
  group_by(Team, grouping) %>%
  summarise(
    shootingPCT = mean(TeamFGPCT)
  )
```
How do you write a mutate step to calulate the percent change when they're stacked on top of each other like that? Answer: You don't. You have to move the data around. 

To take long data and make it wide, we need to `spread` the data. That's the new verb. To spread the data, we tell spread what the new columns will be and what the data values that will go with them. Spread does the rest. 

So I'm going to take that same code and add spread to the bottom. I want the new columns to be my `grouping` and the data to be the `shootingPCT`.

```{r}
nebraska %>% 
  mutate(grouping = case_when(
    Game <=10 ~ "First 10",
    Game >= (max_games - 10) ~ "Last 10")
    ) %>%
  group_by(Team, grouping) %>%
  summarise(
    shootingPCT = mean(TeamFGPCT)
  ) %>% 
  spread(grouping, shootingPCT) 
```

And now, with it spread out, I can chain another mutate step onto it, adding my difference and my percent change. 

```{r}
nebraska %>% 
  mutate(grouping = case_when(
    Game <=10 ~ "First 10",
    Game >= (max_games - 10) ~ "Last 10")
    ) %>%
  group_by(Team, grouping) %>%
  summarise(
    shootingPCT = mean(TeamFGPCT)
  ) %>% 
  spread(grouping, shootingPCT) %>%
  mutate(
    change = ((`Last 10`-`First 10`)/`First 10`)*100,
    difference = (`First 10` - `Last 10`)*100
    )
```
So, over Nebraska's first 10 games, they shot almost 48 percent, where over the last ten horrid games they're shooting 42 percent. That's an 5.7 percentage point different or a nearly 12 percent drop from those first 10 games. 

We can't shoot the ball anymore.

How bad is that nationally? Just run the same code, but instead of Nebraska, use the logs dataframe.

```{r}
logs %>% 
  mutate(grouping = case_when(
    Game <=10 ~ "First 10",
    Game >= (max_games - 10) ~ "Last 10")
    ) %>%
  group_by(Team, grouping) %>%
  summarise(
    shootingPCT = mean(TeamFGPCT)
  ) %>% 
  spread(grouping, shootingPCT) %>%
  mutate(
    change = ((`Last 10`-`First 10`)/`First 10`)*100,
    difference = (`First 10` - `Last 10`)*100
    ) %>% arrange(change)
```

In all of college basketball, Nebraska had the 40th biggest drop in shooting percentage from their first 10 games to their last 10 games. 

And that's a tiny portion of why Tim Miles is no longer the coach.

## Making wide data long

We can reverse this process as well. If we get data that's wide and we want to make it long, we use `gather`. So first, since my data is already long, let me fake a wide dataset using what I just did. 

```{r}
gatherdata <- nebraska %>% 
  mutate(grouping = case_when(
    Game <=10 ~ "First 10",
    Game >= (max_games - 5) ~ "Last 5")
    ) %>%
  group_by(Team, grouping) %>%
  summarise(
    shootingPCT = mean(TeamFGPCT)
  ) %>% 
  spread(grouping, shootingPCT)
```

```{r}
head(gatherdata)
```

Oh drat, I have wide data and for a visualization project, I need long data. Whatever will I do? This might seem silly, but two assignments from now, you're going to need long data from wide and wide data from long, so it's good to know. 

Gather, unfortunately, isn't as easy as spread. It can take some fiddling to get right. There's a ton of examples online if you Google for them, but here's what you do: You tell `gather` what the new column of data you are creating out of the field names first -- this is called the key -- and you then tell it what the value field is going to be called, which is usually a number. Have more than one thing to name your stuff with? After your key value pair, add it with a negative sign in front of it. 

```{r}
gatherdata %>% gather(grouping, shootingPCT, -Team)
```

And just like that, we're back where we started. 

## Why this matters

This matters because certain visualization types need wide or long data. A significant hurdle you will face for the rest of the semester is getting the data in the right format for what you want to do. 

So let me walk you through an example using this data. If I asked you to describe Nebraska's shooting performance over the season, we can do that by what we just did -- grouping them into the first 10 games when people actually talked about this team in the Sweet 16 and the last five when we aren't going to get an NIT bid. We can look at that shooting percentage, how it has changed, and that work makes for a perfect paragraph to write out. So to do that, you need wide data. 

But to visualize it, you need long. 

First, let's load our charting library, `ggplot2` which you're going to learn a lot more about soon.

```{r}
library(ggplot2)
```

Now I'm going to use that data and put the date of the game on the x axis, the shooting percentage on the y axis and then, for giggles, I'm going to add a best fit line that describes our season using a simple regression and the equation of a line. 

```{r}
ggplot(nebraska, aes(x=Date, y=TeamFGPCT)) + 
  geom_smooth(method='lm', se=FALSE, color="grey") + 
  geom_line() + 
  annotate("text", x=(as.Date("2018-12-08")), y=0.545, label="vs Creighton") + 
  annotate("text", x=(as.Date("2019-01-10")), y=0.502, label="vs Penn State") + 
  geom_point(aes(color = W_L)) + 
  scale_y_continuous(labels = scales::percent) + 
  labs(x="Date", y="FG percentage", title="Nebraska's season, visualized", subtitle="As the season goes by, the Huskers are getting worse at shooting the ball.", caption="Source: NCAA | By Matt Waite", color = "Outcome") +
  theme_minimal() + 
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 10),
    axis.title.y = element_blank(),
    axis.text = element_text(size = 7),
    axis.ticks = element_blank(),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),
    legend.position="bottom"
  )
```
Oy. There it is. There's our season.

So I can tell you now, using wide data, that Nebraska's shooting performance over the last ten games is down 12 percent from the first 10 games. And using long data, I can show you the story of the season, but without any specific stats to back it up. 


<!--chapter:end:06-transforming.Rmd-->

# Simulations

Two seasons ago, James Palmer Jr. shot 139 three point attempts and made 43 of them for a .309 shooting percentage last year. A few weeks into last season, he was 7 for 39 -- a paltry .179. Is something wrong or is this just bad luck? Let's simulate 39 attempts 1000 times with his season long shooting percentage and see if this could just be random chance or something else. 

We do this using a base R function called `rbinom` or binomial distribution -- another name for a normal distribution. So what that means is there's a normally distrubuted chance that James Palmer Jr. is going to shoot above and below his career three point shooting percentage. If we randomly assign values in that distribution 1000 times, how many times will it come up 7, like this example?  

```{r}
set.seed(1234)

simulations <- rbinom(n = 1000, size = 39, prob = .309)

hist(simulations)

table(simulations)
```

So what we see is given his season long shooting percentage, it's not out of the realm of randomness that with just 39 attempts for Palmer, he's only hit only 7. In 1000 simulations, it comes up 35 times. Is he below where he should be? Yes. Will he likely improve and soon? Yes. 

## Cold streaks

During the Western Illinois game, the team, shooting .329 on the season from behind the arc, went 0-15 in the second half. How strange is that? 

```{r}
set.seed(1234)

simulations <- rbinom(n = 1000, size = 15, prob = .329)

hist(simulations)

table(simulations)
```
Short answer: Really weird. If you simulate 15 threes 1000 times, sometimes you'll see them miss all of them, but only a few times -- five times, in this case. Most of the time, the team won't go 0-15 even once. So going ice cold is not totally out of the realm of random chance, but it's highly unlikely.


<!--chapter:end:07-simulations.Rmd-->

# Correlations and regression

Throughout sports, you will find no shortage of opinions. From people yelling at their TV screens to an entire industry of people paid to have opinions, there are no shortage of reasons why this team sucks and that player is great. They may have their reasons, but a better question is, does that reason really matter? 

Can we put some numbers behind that? Can we prove it or not? 

This is what we're going to start to answer. And we'll do it with correlations and regressions.

First, we need libraries and [data](https://unl.box.com/s/zlxoptqixkt98gubk3i6316qun99l49r).

```{r}
library(tidyverse)
```

```{r}
correlations <- read_csv("data/correlations.csv")
```

To do this, we need all FBS college football teams and their season stats from last year. How much, over the course of a season, does a thing matter? That's the question you're going to answer. 

In our case, we want to know how much does a team's accumulated penalties influence the number of points they score in a season? How much difference can we explain in points with penalties? 

We're going to use two different methods here and they're closely related. Correlations -- specifically the Pearson Correlation Coefficient -- is a measure of how related two numbers are in a linear fashion. In other words -- if our X value goes up one, what happens to Y? If it also goes up 1, that's a perfect correlation. X goes up 1, Y goes up 1. Every time. Correlation coefficients are a number between 0 and 1, with zero being no correlation and 1 being perfect correlation **if our data is linear**. We'll soon go over scatterplots to visually determine if our data is linear, but for now, we have a hypothesis: More penalties are bad. Penalties hurt. So if a team gets lots of them, they should have worse outcomes than teams that get few of them. That is an argument for a linear relationship between them. 

But is there one?

We're going create a new dataframe called newcorrelations that takes our data that we imported and adds a column called `differential` because we don't have separate offense and defense penalties, and then we'll use correlations to see how related those two things are.

```{r}
newcorrelations <- correlations %>% 
  mutate(differential = OffPoints - DefPoints)
```

In R, there is a `cor` function, and beacuse it's base R, we have to use a different method of referencing fields that we've not used to this point. It involves the name of the data frame plus a `$` then the name of the field. So we want to see if `differential` is correlated with `Yards`, which is the yards of penalties a team gets in a game. We do that by referenceing `newcorrelation$differential` and `newcorrelation$Yards`. The number we get back is the correlation coefficient.

```{r}
cor(newcorrelations$differential, newcorrelations$Yards, method="pearson")
```

So on a scale of 0 to 1, penalty yards and wether or not the team scores more points than it give up are at .2. You could say they're 20 percent related. Another way to say it? They're 80 percent not related.

What about the number of penalties instead of the yards?

```{r}
cor(newcorrelations$differential, newcorrelations$`Pen.`, method="pearson")
```

Even less related. What about looking at the average? Penalty yards per game?

```{r}
cor(newcorrelations$differential, newcorrelations$`Pen./G`, method="pearson")
```

Not only is it less related, but the relationship is inverted. 

So wait, what does that mean? 

It means that the number of penalty yards and penalties is actually positively related to differential. Put another way, teams that have more penalties and penalty yards tend to have better outcomes. The average is barely -- 3 percent -- negatively correlated, meaning that teams with higher averages score fewer points.

What? That makes no sense. How can that be? 

Enter regression. Regression is how we try to fit our data into a line that explains the relationship the best. Regressions will help us predict things as well -- if we have a team that has so many penalties, what kind of point differential could we expect, given every FBS team? So regressions are about prediction, correlations are about description. Correlations describe a relationship. Regressions help us predict what that relationship means. 

Another thing regressions do is give us some other tools to evaluate if the relationship is real or not.

Here's an example of using linear modeling to look at yards. Think of the `~` character as saying "is predicted by". The output looks like a lot, but what we need is a small part of it.

```{r}
fit <- lm(differential ~ Yards, data = newcorrelations)
summary(fit)
```

There's three things we need here: 

1. First we want to look at the p-value. It's at the bottom right corner of the output. In the case of Yards, the p-value is .02193. The threshold we're looking for here is .05. If it's less than .05, then the relationship is considered to be *statistically significant*. Significance here does not mean it's a big deal. It means it's not random. That's it. Just that. Not random. So in our case, the relationship between penalty yards and a team's aggregate point differential are not random. It's a real relationship.
2. Second, we look at the Adjusted R-squared value. It's right above the p-value. Adjusted R-squared is a measure of how much of the difference between teams aggregate point values can be explained by penalty yards. Our correlation coefficient said they're 20 percent related to each other, but penalty yard's ability to explain the difference between teams? About 3.3 percent. That's ... not much. It's really nothing.
3. The third thing we can look at, and we only bother if the first two are meaningful, is the coefficients. In the middle, you can see the (Intercept) is -108.73848 and the Yards coefficient is .19484. Remember high school algebra? Remember learning the equation of a line? Remember swearing that learning `y=mx+b` is stupid because you'll never need it again? Surprise. It's useful again. In this case, we could try to predict a team's aggregate score in a season -- will they score more than they give up -- by using `y=mx+b`. In this case, y is the aggregate score, m is .19484 and b is -108.73848. So we would multiply a teams total penalty yards by .19484 and then subtract 108.73848 from it. The result would tell you what the total aggregate score in the season would be. Chance that your even close with this? About 3 percent. 

You can see the problem in a graph. On the X axis is penalty yards, on the y is aggregate score. If these elements had a strong relationship, we'd see a clear pattern moving from right to left, sloping down. Onn the left would be the teams with lots of penalty yards and a negative point differential. On right would be teams with low penalty yards and high point differentials. Do you see that below?

```{r echo=FALSE}
ggplot(newcorrelations, aes(x=Yards, y=differential)) + geom_point()
```

> **Your turn**: Try it with the other penalty measures. Total penalties and penalty yards per game. Does anything change? Do either of these meet the .05 threshold for randomness? Are either of these any more predictive?

## A more predictive example

So we've firmly established that penalties aren't predictive. But what is? One measure I've found to  be highly predictive of a team's success is how well do they do on third down. It's simple really: Succeed on third down, you get to stay on offense. Fail on third down, you are punting (most likely) or settling for a field goal. Either way, you're scoring less than you would by scoring touchdowns. How related are points per game and third down conversion percentage?

```{r}
cor(newcorrelations$OffPointsG, newcorrelations$OffConversionPct, method="pearson")
```

Answer: 67 percent. More than three times more related than penalty yards. But how meaningful is that relationship and how predictive is it?

```{r}
third <- lm(OffPointsG ~ OffConversionPct, data = newcorrelations)
summary(third)
```

First we check p-value. See that e-16? That means scientific notation. That means our number is 2.2 times 10 to the -16 power. So -.000000000000000022. That's sixteen zeros between the decimal and 22. Is that less than .05? Uh, yeah. So this is really, really, really not random. But anyone who has watched a game of football knows this is true. It makes intuitive sense. 

Second, Adjusted R-squared: .4391. So we can predict 44 percent of the difference in the total offensive points per game a team scores by simply looking at their third down conversion percentage. 
Third, the coefficients: In this case, our `y=mx+b` formula looks like `y = .85625x-4.74024`. So if we were applying this, let's look at Nebraska's 31-28 loss to Iowa on Black Friday in 2018. Nebraska was 6-15 on third down in that game, or 40 percent (Iowa was 7 of 13 or 54 percent). Given those numbers, our formula predicts Nebraska should have scored how many points? 

```{r}
(0.85625 * 40) - 4.74024 
```
That's really close to the 28 they did score. And Iowa?
```{r}
(0.85625 * 54) - 4.74024 
```
By our model, Iowa should have scored 10 more points than they did. But they didn't. Why, besides Iowa is terrible and deserves punishment from the football gods for being Iowa? Remember our model can only explain 44 percent of the points. There's more to football than one metric. 

<!--chapter:end:08-correlation.Rmd-->

# Multiple regression

Last chapter, we looked at correlations and linear regression to predict how one element of a game would predict the score. But we know that a single variable, in all but the rarest instances, are not going to be that predictive. We need more than one. Enter multiple regression. Multiple regression lets us add -- wait for it -- mulitiple predictors to our equation to help us get a better 

That presents it's own problems. So let's get our libraries and our data, this time of [every college basketball game since the 2014-15 season](https://unl.box.com/s/u9407jj007fxtnu1vbkybdawaqg6j3fw) loaded up. 

```{r}
library(tidyverse)
```

```{r}
logs <- read_csv("data/logs1519.csv")
```

So one way to show how successful a basketball team was for a game is to show the differential between the team's score and the opponnent's score. Score a lot more than the opponent = good, score a lot less than the opponent = bad. And, relatively speaking, the more the better. So let's create that differential.

```{r}
logs <- logs %>% mutate(Differential = TeamScore - OpponentScore)
```

The linear model code we used before is pretty straight forward. Its `field` is predicted by `field`. Here's a simple linear model that looks at predicting a team's point differential by looking at their offensive shooting percentage. 

```{r}
shooting <- lm(TeamFGPCT ~ Differential, data=logs)
summary(shooting)
```

Remember: There's a lot here, but only some of it we care about. What is the Adjusted R-squared value? What's the p-value and is it less than .05? In this case, we can predict 37 percent of the difference in differential with how well a team shoots the ball. 

To add more predictors to this mix, we merely add them. But it's not that simple, as you'll see in a moment. So first, let's look at adding how well the other team shot to our prediction model:

```{r}
model1 <- lm(Differential ~ TeamFGPCT + OpponentFGPCT, data=logs)
summary(model1)
```

First things first: What is the adjusted R-squared?

Second: what is the p-value and is it less than .05? 

Third: Compare the residual standard error. We went from .05949 to 9.4. The meaning of this is both really opaque and also simple -- we added a lot of error to our model by adding more measures -- 158 times more. Residual standard error is the total distance between what our model would predict and what we actually have in the data. So lots of residual error means the distance between reality and our model is wider. So the width of our predictive range in this example grew pretty dramatically, but so did the amount of the difference we could predict. It's a trade off. 

One of the more difficult things to understand about multiple regression is the issue of multicollinearity. What that means is that there is significant correlation overlap between two variables -- the two are related to each other as well as to the target output -- and all you are doing by adding both of them is adding error with no real value to the R-squared. In pure statistics, we don't want any multicollinearity at all. Violating that assumption limits the applicability of what you are doing. So if we have some multicollinearity, it limits our scope of application to college basketball. We can't say this will work for every basketball league and level everywhere. What we need to do is see how correlated each value is to each other and throw out ones that are highly co-correlated.

So to find those, we have to create a correlation matrix that shows us how each value is correlated to our outcome variable, but also with each other. We can do that in the `Hmisc` library. We install that in the console with `install.packages("Hmisc")`

```{r}
library(Hmisc)
```

We can pass in every numeric value to the Hmisc library and get a correlation matrix out of it, but since we have a large number of values -- and many of them character values -- we should strip that down and reorder them. So that's what I'm doing here. I'm saying give me differential first, and then columns 9-24, and then 26-41. Why the skip? There's a blank column in the middle of the data -- a remnant of the scraper I used. 

```{r}
simplelogs <- logs %>% select(Differential, 9:24, 26:41)
```

Before we proceed, what we're looking to do is follow the Differential column down, looking for correlation values near 1 or -1. Correlations go from -1, meaning perfect negative correlation, to 0, meaning no correlation, to 1, meaning perfect positive correlation. So we're looking for numbers near 1 or -1 for their predictive value. BUT: We then need to see if that value is also highly correlated with something else. If it is, we have a decision to make.

We get our correlation matrix like this:

```{r}
cormatrix <- rcorr(as.matrix(simplelogs))

cormatrix$r
```

Notice right away -- TeamFG is highly correlated. But it's also highly correlated with TeamFGPCT. And that makes sense. A team that doesn't shoot many shots is not going to have a high score differential. But the number of shots taken and the field goal percentage are also highly related. So including both of these measures would be pointless -- they would add error without adding much in the way of predictive power. 

> **Your turn**: What else do you see? What other values have predictive power and aren't co-correlated? 

We can add more just by simply adding them. 

```{r}
model2 <- lm(Differential ~ TeamFGPCT + OpponentFGPCT + TeamTotalRebounds + OpponentTotalRebounds, data=logs)
summary(model2)
```

Go down the list:

What is the Adjusted R-squared now? 
What is the p-value and is it less than .05?
What is the Residual standard error? 

The final thing we can do with this is predict things. Look at our coefficients table. See the Estimates? We can build a formula from that, same as we did with linear regressions.

```
Differential = (TeamFGPCT*100.880013) + (OpponentFGPCT*-97.563291) + (TeamTotalRebounds*0.516176) + (OpponentTotalRebounds*-0.436402) - 3.655461
```

How does this apply in the real world? Let's pretend for a minute that you are Fred Hoiberg, and you  have just been hired as Nebraska's Mens Basketball Coach. Your job is to win conference titles and go deep into the NCAA tournament. To do that, we need to know what attributes of a team should we emphasize. We can do that by looking at what previous Big Ten conference champions looked like.

So if our goal is to predict a conference champion team, we need to know what those teams did. Here's the regular season conference champions in this dataset. 

```{r}
logs %>% filter(Team == "Michigan State Spartans" & season == "2018-2019" | Team == "Michigan State Spartans" & season == "2017-2018" | Team == "Purdue Boilermakers" & season == "2016-2017" | Team == "Indiana Hoosiers" & season == "2015-2016" | Team == "Wisconsin Badgers" & season == "2014-2015") %>% summarise(avgfgpct = mean(TeamFGPCT), avgoppfgpct=mean(OpponentFGPCT), avgtotrebound = mean(TeamTotalRebounds), avgopptotrebound=mean(OpponentTotalRebounds))
```

Now it's just plug and chug. 

```{r}
(0.4886133*100.880013) + (0.4090221*-97.563291) + (35.29834*0.516176) + (27.20994*-0.436402) - 3.655461
```

So a team with those numbers is going to average scoring 12 more points per game than their opponent.

How does that compare to Nebraska of this past season? The last of the Tim Miles era? 

```{r}
logs %>% filter(Team == "Nebraska Cornhuskers" & season == "2018-2019") %>% summarise(avgfgpct = mean(TeamFGPCT), avgoppfgpct=mean(OpponentFGPCT), avgtotrebound = mean(TeamTotalRebounds), avgopptotrebound=mean(OpponentTotalRebounds))
```

```{r}
(0.4305833*100.880013) + (0.4226667*-97.563291) + (32.5*0.516176) + (34.94444*-0.436402) - 3.655461
```

By this model, it predicted we would outscore our opponent by .07 points over the season. So we'd win slightly more than we'd lose. Nebraska's overall record? 19-17. 

<!--chapter:end:09-multipleregression.Rmd-->

# Residuals

When looking at a linear model of your data, there's a measure you need to be aware of called residuals. The residual is the distance between what the model predicted and what the real outcome is. So if your model predicted a team would score 38 points per game given their third down conversion percentage, and they score 45, then your residual is 7. If they had scored 31, then their residual would be -7. 

Residuals can tell you severals things, but most importantly is if a linear model the right model for your data. If the residuals appear to be random, then a linear model is appropriate. If they have a pattern, it means something else is going on in your data and a linear model isn't appropriate. 

Residuals can also tell you who is underperforming and overperforming the model. Let's take a look at an example we've used regularly this semester -- third down conversion percentage and penalties. 

Let's first attach libraries and use rvest to get some data. Note: In the rvest steps, I rename the first column because it's blank on the page and then I merge scoring offense to two different tables -- third downs and penalties. 

```{r}
library(tidyverse)
```

```{r}
offense <- read_csv("data/correlations.csv")
```

First, let's build a linear model and save it as a new dataframe called `fit`. 

```{r}
fit <- lm(`OffPointsG` ~ `OffConversionPct`, data = offense)
summary(fit)
```

We've seen this output before, but let's review because if you are using scatterplots to make a point, you should do this. First, note the Min and Max residual at the top. A team has underperformed the model by 11.4 points, and a team has overperformed it by 13.5. The median residual, where half are above and half are below, is just slightly under the fit line. Close here is good. 

Next: Look at the Adjusted R-squared value. What that says is that 44 percent of a team's scoring output can be predicted by their third down conversion percentage. This is just one year, so that's a little low. If we did this with more years, that would go up. 

Last: Look at the p-value. We are looking for a p-value smaller than .05. At .05, we can say that our correlation didn't happen at random. And, in this case, it REALLY didn't happen at random. 

What we want to do now is look at those residuals. We can add them to our dataframe like this:

```{r}
offense$predicted <- predict(fit)
offense$residuals <- residuals(fit)
```

Now we can sort our data by those residuals. Sorting in descending order gives us the teams that are overperforming the model.

```{r}
offense %>% arrange(desc(residuals))
```
So looking at this table, what you see here are the teams who scored more than their third down conversion percentage would indicate. Some of those teams were just lucky. Some of those teams were really good at long touchdown plays that didn't need a lot of third downs to get down the field. But these are your overperformers. 

But, before we can bestow any validity on it, we need to see if this linear model is appropriate. We've done that some looking at our p-values and R-squared values. But one more check is to look at the residuals themselves. We do that by plotting the residuals with the predictor. We'll get into plotting soon, but for now just seeing it is enough.

```{r echo=FALSE}
ggplot(offense, aes(x=OffConversionPct, y=residuals)) + geom_point()
```

The lack of a shape here -- the seemingly random nature -- is a good sign that a linear model works for our data. If there was a pattern, that would indicate something else was going on in our data and we needed a different model.

Another way to view your residuals is by connecting the predicted value with the actual value.

```{r echo=FALSE}
ggplot(data=offense, aes(x=OffConversionPct, y=OffPointsG)) + geom_point() + geom_segment(aes(xend = OffConversionPct, yend = predicted)) + geom_smooth(method=lm, se=FALSE)
```

The blue line here separates underperformers from overperformers.

## Penalties

Now let's look at it where it doesn't work: Penalties. 

```{r}
penalties <- offense
```


```{r}
pfit <- lm(OffPointsG ~ Yards, data = penalties)
summary(pfit)
```

So from top to bottom:

* Our min and max go from -16.5 to positive 17.1
* Our adjusted R-squared is ... .06. Not much at all. 
* Our p-value is ... .002, which is less than than .05. 

So what we can say about this model is that it's statistically significant but utterly meaningless. Normally, we'd stop right here -- why bother going forward with a predictive model that isn't predictive? But let's do it anyway. 

```{r}
penalties$predicted <- predict(pfit)
penalties$residuals <- residuals(pfit)
```

```{r}
penalties %>% arrange(desc(residuals))
```

So our model says Oklahoma *should* only be scoring 31.3 points per game given how many penalty yards per game, but they're really scoring 48.4. Oy. What happens if we plot those residuals? 

```{r echo=FALSE}
ggplot(penalties, aes(x=Yards, y=residuals)) + geom_point()
```

Well ... it actually says that a linear model is appropriate. Which an important lesson -- just because your residual plot says a linear model works here, that doesn't say your linear model is good. There are other measures for that, and you need to use them. 

Here's the segment plot of residuals -- you'll see some really long lines. That's a bad sign. 

```{r echo=FALSE}
ggplot(data=penalties, aes(x=Yards, y=OffPointsG)) + geom_point() + geom_segment(aes(xend = Yards, yend = predicted)) + geom_smooth(method=lm, se=FALSE)
```


<!--chapter:end:10-residuals.Rmd-->

# Z scores

Z scores are a handy way to standardize scores so you can compare things across groupings. In our case, we may want to compare teams by year, or era. We can use z scores to answer questions like who was the greatest X of all time, because a Z score can put them in context to their era. 

We can also use z scores to ask how much better is team A from team B. 

So let's use Nebraska basketball, which if you haven't been reading lately is at a bit of a crossroads. 

A Z score is a measure of how far a number is from the population mean of that number. An easier way to say that -- how different is my grade from the average grade in the class. The formula for calculating a Z score is `(MyScore - AverageScore)/Standard Deviation of Scores`. The standard deviation is a number calculated to show the amount of variation in a set of data. In a normal distribution, 68 percent of all scores will be within 1 standard deviation, 95 percent will be within 2 and 99 within 3. 

## Calculating a Z score in R

```{r}
library(tidyverse)
```

```{r}
gamelogs <- read_csv("data/logs19.csv")
```

The first thing we need to do is select some fields we think represent team quality:

```{r}
teamquality <- gamelogs %>% select(Conference, Team, TeamFGPCT, TeamTotalRebounds, OpponentFGPCT)
```

And since we have individual game data, we need to collapse this into one record for each team. We do that with ... group by.

```{r}
teamtotals <- teamquality %>% 
  group_by(Conference, Team) %>% 
  summarise(
    FGAvg = mean(TeamFGPCT), 
    ReboundAvg = mean(TeamTotalRebounds), 
    OppFGAvg = mean(OpponentFGPCT)
    )
```

To calculate a Z score in R, the easiest way is to use the scale function in base R. To use it, you use `scale(FieldName, center=TRUE, scale=TRUE)`. The center and scale indicate if you want to subtract from the mean and if you want to divide by the standard deviation, respectively. We do.

When we have multiple Z Scores, it's pretty standard practice to add them together into a composite score. That's what we're doing at the end here with `TotalZscore`. Note: We have to invert OppZscore by multiplying it by a negative 1 because the lower someone's opponent shooting percentage is, the better. 

```{r}
teamzscore <- teamtotals %>% mutate(
  FGzscore = as.numeric(scale(FGAvg, center = TRUE, scale = TRUE)),
  RebZscore = as.numeric(scale(ReboundAvg, center = TRUE, scale = TRUE)),
  OppZscore = as.numeric(scale(OppFGAvg, center = TRUE, scale = TRUE)) * -1,
  TotalZscore = FGzscore + RebZscore + OppZscore
  )  
```

So now we have a dataframe called `teamzscore` that has 353 basketball teams with Z scores. What does it look like? 

```{r}
head(teamzscore)
```

A way to read this -- a team at zero is precisely average. The larger the positive number, the more exceptional they are. The larger the negative number, the more truly terrible they are. 

So who are the best teams in the country? 

```{r}
teamzscore %>% arrange(desc(TotalZscore))
```

Don't sleep on South Dakota State come tournament time!

But closer to home, how is Nebraska doing.

```{r}
teamzscore %>% filter(Conference == "Big Ten") %>% arrange(desc(TotalZscore))
```

So, as we can see, with our composite Z Score, Nebraska is ... not bad, but not good either: 9 of 14 teams in the Big Ten.

<!--chapter:end:11-zscores.Rmd-->

# Intro to ggplot

With `ggplot2`, we dive into the world of programmatic data visualization. The `ggplot2` library implements something called the grammar of graphics. The main concepts are: 

* aesthetics - which in this case means the data which we are going to plot
* geometries - which means the shape the data is going to take
* scales - which means any transformations we might make on the data
* facets - which means how we might graph many elements of the same dataset in the same space
* layers - which means how we might lay multiple geometries over top of each other to reveal new information.

Hadley Wickam, who is behind all of the libaries we have used in this course to date, wrote about his layered grammar of graphics in [this 2009 paper that is worth your time to read](http://byrneslab.net/classes/biol607/readings/wickham_layered-grammar.pdf). 

Here are some `ggplot2` resources you'll want to keep handy: 

* [The ggplot documentation](http://ggplot2.tidyverse.org/reference/index.html).
* [The ggplot cookbook](http://www.cookbook-r.com/Graphs/)

Let's dive in using data we've already seen before -- football attendance. This workflow will represent a clear picture of what your work in this class will be like for much of the rest of the semester. One way to think of this workflow is that your R Notebook is now your digital sketchbook, where you will try different types of visualizations to find ones that work. Then, you will export your work into a program like Illustrator to finish the work. 

To begin, we'll import the `ggplot2` and `dplyr` libraries. We'll read in the data, then create a new dataframe that represents our attendance data, similar to what we've done before. 

```{r}
library(tidyverse)
```

```{r}
attendance <- read_csv('data/attendance.csv')
```

First, let's get a top 10 list by announced attendance this last season. We'll use the same tricks we used in the filtering assignment. 

```{r}
attendance %>% arrange(desc(`2018`)) %>% top_n(10) %>% select(Institution, `2018`)
```

That looks good, so let's save it to a new data frame and use that data frame instead going forward.

```{r}
top10 <- attendance %>% arrange(desc(`2018`)) %>% top_n(10) %>% select(Institution, `2018`)
```

The easiest thing we can do is create a simple bar chart of our data. We could, for instance, create a bar chart of the total attendance. To do that, we simply tell `ggplot2` what our dataset is, what element of the data we want to make the bar chart out of (which is the aesthetic), and the geometry type (which is the geom). It looks like this:

`ggplot(top10, aes(x=Institution)) + geom_bar()` 

Note: attendace is our data, `aes` means aesthetics, `x=Institution` explicitly tells `ggplot2` that our x value -- our horizontal value -- is the Instituition field from the data, and then we add on the `geom_bar()` as the geometry. And what do we get when we run that? 

```{r}
ggplot(top10, aes(x=Institution)) + geom_bar()
```

We get ... weirdness. We expected to see bars of different sizes, but we get all with a count of 1. What gives? Well, this is the default behavior. What we have here is something called a histogram, where `ggplot2` helpfully counted up the number of times the Institution appears and counted them up. Since we only have one record per Institution, the count is always 1. How do we fix this? By adding `weight` to our aesthetic. 

```{r}
ggplot(top10, aes(x=Institution, weight=`2018`)) + geom_bar()
```

Closer. But ... what order is that in? And what happened to our count numbers on the left? Why are they in scientific notation?

Let's deal with the ordering first. `ggplot2`'s default behavior is to sort the data by the x axis variable. So it's in alphabetical order. To change that, we have to `reorder` it. With `reorder`, we first have to tell `ggplot` what we are reordering, and then we have to tell it HOW we are reordering it. So it's reorder(FIELD, SORTFIELD). 

```{r}
ggplot(top10, aes(x=reorder(Institution, `2018`), weight=`2018`)) + geom_bar()
```

Better. We can argue about if the right order is smallest to largest or largest to smallest. But this gets us close. By the way, to sort it largest to smallest, put a negative sign in front of the sort field. 

```{r}
ggplot(top10, aes(x=reorder(Institution, -`2018`), weight=`2018`)) + geom_bar()
```

## Scales

To fix the axis labels, we need try one of the other main elements of the `ggplot2` library, which is transform a scale. More often that not, that means doing something like putting it on a logarithmic scale or soem other kind of transformation. In this case, we're just changing how it's represented. The default in `ggplot2` for large values is to express them as scientific notation. Rarely ever is that useful in our line of work. So we have to transform them into human readable numbers. 

The easiest way to do this is to use a library called `scales` and it's already installed.

```{r}
library(scales)
```

To alter the scale, we add a piece to our plot with `+` and we tell it which scale is getting altered and what kind of data it is. In our case, our Y axis is what is needing to be altered, and it's continuous data (meaning it can be any number between x and y, vs discrete data which are categorical). So we need to add `scale_y_continuous` and the information we want to pass it is to alter the labels with a function called `comma`. 

```{r}
ggplot(top10, aes(x=reorder(Institution, -`2018`), weight=`2018`)) + geom_bar() + scale_y_continuous(labels=comma)

```

Better. 

## Styling

We are going to spend a lot more time on styling, but let's add some simple labels to this with a new bit called `labs` which is short for labels. 

```{r}
ggplot(top10, aes(x=reorder(Institution, -`2018`), weight=`2018`)) + geom_bar() + scale_y_continuous(labels=comma) + labs(title="Top 10 Football Programs By Attendance", x="School", y="Attendance")
```

The library has lots and lots of ways to alter the styling -- we can programmatically control nearly every part of the look and feel of the chart. One simple way is to apply themes in the library already. We do that the same way we've done other things -- we add them. Here's the light theme. 

```{r}
ggplot(top10, aes(x=reorder(Institution, -`2018`), weight=`2018`)) + geom_bar() + scale_y_continuous(labels=comma) + labs(title="Top 10 Football Programs By Attendance", x="School", y="Attendance") + theme_light()
```

Or the minimal theme:

```{r}
ggplot(top10, aes(x=reorder(Institution, -`2018`), weight=`2018`)) + geom_bar() + scale_y_continuous(labels=comma) + labs(title="Top 10 Football Programs By Attendance", x="School", y="Attendance") + theme_minimal()
```

Later on, we'll write our own themes. For now, the built in ones will get us closer to something that looks good.

## One last trick: coord flip

Sometimes, we don't want vertical bars. Maybe we think this would look better horizontal. How do we do that? By adding `coord_flip()` to our code. It does what it says -- it inverts the coordinates of the figures.

```{r}
ggplot(top10, aes(x=reorder(Institution, -`2018`), weight=`2018`)) + geom_bar() + scale_y_continuous(labels=comma) + labs(title="Top 10 Football Programs By Attendance", x="School", y="Attendance") + theme_minimal() + coord_flip()
```

<!--chapter:end:12-ggplot.Rmd-->

# Stacked bar charts

One of the elements of data visualization excellence, accoring to Tufte, is inviting comparison. Often that comes in showing what proportion a thing is in relation to the whole thing. With bar charts, if we have information about the parts of the whole, we can stack them on top of each other to compare them. And it's a simple change to what we've already done. 

```{r}
library(tidyverse)
```
We're going to use a dataset of graduation rates by gender by school in the NCAA. [You can get it here](https://unl.box.com/s/3nw1eokvs9zfdjyzvjaj3xdq01rm8sym). 

```{r}
grads <- read_csv('data/grads.csv')
```
What we have here is the name of the school, the conference, the cohort of when they started school, the gender, the number of that gender that graduated and the total number of graduates in that cohort. 

Let's pretend for a moment we're looking at the graduation rates of men and women in the Big 10 Conference and we want to chart that. First, let's work on our data. We need to filter the "Big Ten Conference" school, and we want the latest year, which is 2009. So we'll create a dataframe called `BIG09` and populate it. 

```{r}
BIG09 <- grads %>% filter(`Primary Conference in Actual Year`=="Big Ten Conference") %>% filter(`Cohort year` == 2009)
```

```{r}
head(BIG09)
```

Building on what we learned in the last chapter, we know we can turn this into a bar chart with an x value, a weight and a geom_bar. What're going to add is a `fill`. The `fill` will stack bars on each other based on which element it is. In this case, we can fill the bar by Gender, which means it will stack the number of male graduates on top of the number of female graduates and we can see how they compare. 

```{r}
ggplot(BIG09, aes(x=reorder(`Institution name`, -Total), weight=`Number of completers`, fill=Gender)) + geom_bar() + coord_flip()
```

What's the problem with this chart? 

Let me ask a different question -- which schools have larger differences in male and female graduation rates? Can you compare Illnois to Northwestern? Not really. We've charted the total numbers. We need the percentage of the whole. 

> **YOUR TURN**: Using what you know -- hint: mutate -- how could you chart this using percents of the whole instead of counts? 

<!--chapter:end:13-stackedbar.Rmd-->

# Waffle charts

Pie charts are the devil. They should be an instant F in any data visualization class. I'll give you an example of why.

What's the racial breakdown of journalism majors at UNL?

Here it is in a pie chart:

```{r}
library(tidyverse)

enrollment <- read.csv("~/Dropbox/JOUR407-Data-Visualization/Data/collegeenrollment.csv")

jour <- filter(enrollment, MajorName == "Journalism")

jdf <- jour %>% 
group_by(Race) %>%
summarise(
       total=sum(Count)) %>%
select(Race, total) %>% 
filter(total != 0)

ggplot(jdf, aes(x="", y=total, fill=Race)) + geom_bar(width = 1, stat = "identity") + coord_polar("y", start=0)
```
You can see, it's pretty white. But ... what about beyond that? How carefully can you evaluate angles and area?

Not well.

So let's introduce a better way: The Waffle Chart. Some call it a square pie chart. I personally hate that. Waffles it is. 

First, install the library in the console: 

`install.packages('waffle')`

Now load it: 

```{r}
library(waffle)
```

Let's look at the debacle in Ann Arbor with Nebraska basketball. [Here's the box score](https://www.sports-reference.com/cbb/boxscores/2019-02-28-19-michigan.html), which we'll use for this walkthrough. 

The easiest way to do waffle charts is to make vectors of your data and plug them in. To make a vector, we use the `c` or concatenate function, something we've done before. 

So let's look at ... shooting. We shot like crap that night, they didn't. 

```{r}
nu <- c("Made"=23, "Missed"=44)
mi <- c("Made"=30, "Missed"=24)
```

So what does the breakdown of the night look like?

The waffle library can break this down in a way that's easier on the eyes than a pie chart. We call the library, add the data, specify the number of rows, give it a title and an x value label, and to clean up a quirk of the library, we've got to specify colors. 

```{r}
waffle(nu, rows = 5, title="Nebraska's shooting night", xlab="1 square = 1 shot", colors = c("black", "red"))
```

Or, we could make this two teams in the same chart.

```{r}
game <- c("Nebraska"=23, "Michigan"=30)
```

```{r}
waffle(game, rows = 5, title="Nebraska vs Michigan: made shots", xlab="1 square = 1 shot", colors = c("red", "dark blue"))
```

## Waffle Irons

So what does it look like if we compare the two teams. Do do that -- and I am not making this up -- you have to create a waffle iron. Get it? Waffle charts? Iron? 

```{r}
iron(
 waffle(nu, rows = 5, title="Nebraska's night shooting", xlab="1 square = 1 shot", colors = c("black", "red")),
 waffle(mi, rows = 5, title="Michigan's night shooting", xlab="1 square = 1 shot", colors = c("dark blue", "yellow"))
)
```

What do you notice about this chart? Notice how the squares aren't the same size? Well, Michigan only took 54 shots. We took 67. So the squares aren't the same size because the numbers aren't the same. We can fix that by adding an unnamed padding number so the number of shots add up to the same thing. Let's make the total for everyone be 70. So to do that, we need to add a padding of 3 to Nebraska and a padding of 16 to Michigan. REMEMBER: Don't name it or it'll show up in the legend.

```{r}
nu <- c("Made"=23, "Missed"=44, 3)
mi <- c("Made"=30, "Missed"=24, 16)
```

Now, in our waffle iron, if we don't give that padding a color, we'll get an error. So we need to make it white. Which, given our white background, means it will disappear.

```{r}
iron(
 waffle(nu, rows = 5, title="Nebraska's night shooting", colors = c("black", "red", "white")),
 waffle(mi, rows = 5, title="Michigan's night shooting", xlab="1 square = 1 shot", colors = c("dark blue", "yellow", "white"))
)
```



<!--chapter:end:14-wafflecharts.Rmd-->

# Line charts

Bar charts -- stacked or otherwise -- are good for showing relative size of a thing compared to another thing. Line charts, which we work on here, are good for showing change over time. 

So let's look at how we can answer this question: Why was Nebraska terrible last season?

Let's start getting all that we need. We can use the tidyverse shortcut. 

```{r}
library(tidyverse)
```

Now we'll import the data you created. Mine looks like this: 

```{r}
logs <- read_csv("data/logs19.csv")
```

This data has every game from every team in it, so we need to use filtering to limit it, because we just want to look at Nebraska. If you don't remember, flip back to chapter 5. 

```{r}
nu <- logs %>% filter(Team == "Nebraska Cornhuskers")
```

Because this data has just Nebraska data in it, the dates are formatted correctly, and the data is long data (instead of wide), we have what we need to make line charts.

Line charts, unlike bar charts, do have a y-axis. So in our ggplot step, we have to define what our x and y axes are. In this case, the x axis is our Date -- the most common x axis in line charts is going to be a date of some variety -- and y in this case is up to us. We've seen from previous walkthroughs that how well a team shoots the ball has a lot to do with how well a team does in a season, so let's chart that. 

```{r}
ggplot(nu, aes(x=Date, y=TeamFGPCT)) + geom_line()
```

See a problem here? Note the Y axis doesn't start with zero. That makes this look worse than it is (and that February swoon is pretty bad). To make the axis what you want, you can use `scale_x_continuous` or `scale_y_continuous` and pass in a list with the bottom and top value you want. You do that like this:

```{r}
ggplot(nu, aes(x=Date, y=TeamFGPCT)) + geom_line() + scale_y_continuous(limits = c(0, .6))
```

Note also that our X axis labels are automated. It knows it's a date and it just labels it by month. 
## This is too simple. 

With datasets, we want to invite comparison. So let's answer the question visually. Let's put two lines on the same chart. How does Nebraska compare to Michigan State and Purdue, the eventual regular season co-champions? 

```{r}
msu <- logs %>% filter(Team == "Michigan State Spartans")
```

In this case, because we have two different datasets, we're going to put everything in the geom instead of the ggplot step. We also have to explicitly state what dataset we're using by saying `data=` in the geom step.

First, let's chart Nebraska. Read carefully. First we set the data. Then we set our aesthetic. Unlike bars, we need an X and a Y variable. In this case, our X is the date of the game, Y is the thing we want the lines to move with. In this case, the Team Field Goal Percentage -- TeamFGPCT. 

```{r}
ggplot() + geom_line(data=nu, aes(x=Date, y=TeamFGPCT), color="red")
```

Now, by using +, we can add Michigan State to it. REMEMBER COPY AND PASTE IS A THING. Nothing changes except what data you are using.

```{r}
ggplot() + geom_line(data=nu, aes(x=Date, y=TeamFGPCT), color="red") + geom_line(data=msu, aes(x=Date, y=TeamFGPCT), color="dark green")
```

Let's flatten our lines out by zeroing the Y axis.

```{r}
ggplot() + geom_line(data=nu, aes(x=Date, y=TeamFGPCT), color="red") + geom_line(data=msu, aes(x=Date, y=TeamFGPCT), color="dark green") + scale_y_continuous(limits = c(0, .6))
```

So visually speaking, the difference between Nebraska and Michigan State's season is that Michigan State stayed mostly on an even keel, and Nebraska went on a two month swoon.

## But what if I wanted to add a lot of lines. 

Fine. How about all Power Five Schools? This data for example purposes. You don't have to do it. 

```{r}
powerfive <- c("SEC", "Big Ten", "Pac-12", "Big 12", "ACC")

p5conf <- logs %>% filter(Conference %in% powerfive)
```

I can keep layering on layers all day if I want. And if my dataset has more than one team in it, I need to use the `group` command. And, the layering comes in order -- so if you're going to layer a bunch of lines with a smaller group of lines, you want the bunch on the bottom. So to do that, your code stacks from the bottom. The first geom in the code gets rendered first. The second gets layered on top of that. The third gets layered on that and so on. 

```{r}
ggplot() + geom_line(data=p5conf, aes(x=Date, y=TeamFGPCT, group=Team), color="light grey") + geom_line(data=nu, aes(x=Date, y=TeamFGPCT), color="red") + geom_line(data=msu, aes(x=Date, y=TeamFGPCT), color="dark green") + scale_y_continuous(limits = c(0, .6))
```

What do we see here? How has Nebraska and Michigan State's season evolved against all the rest of the teams in college basketball?

But how does that compare to the average? We can add that pretty easily by creating a new dataframe with it and add another geom_line. 

```{r}
average <- logs %>% group_by(Date) %>% summarise(mean_shooting=mean(TeamFGPCT))
```

```{r}
ggplot() + geom_line(data=p5conf, aes(x=Date, y=TeamFGPCT, group=Team), color="light grey") + geom_line(data=nu, aes(x=Date, y=TeamFGPCT), color="red") + geom_line(data=msu, aes(x=Date, y=TeamFGPCT), color="dark green") + geom_line(data=average, aes(x=Date, y=mean_shooting), color="black") + scale_y_continuous(limits = c(0, .6))
```

<!--chapter:end:15-linecharts.Rmd-->

# Step charts

Step charts are a method of showing progress toward something. There's been two great examples lately. First is the Washignton Post looking at [Lebron passing Jordan's career point total](https://www.washingtonpost.com/graphics/sports/lebron-james-michael-jordan-nba-scoring-list/?utm_term=.481074150849). Another is John Burn-Murdoch's work at the Financial Times (which is paywalled) about soccer stars. [Here's an example of his work outside the paywall](http://johnburnmurdoch.github.io/projects/goal-lines/CL/).

To replicate this, we need cumulative data -- data that is the running total of data at a given point. So think of it this way -- Nebraska scores 50 points in a basketball game and then 50 more the next, their cumulative total at two games is 100 points. 

Step charts can be used for all kinds of things -- showing how a player's career has evolved over time, how a team fares over a season, or franchise history. Let's walk through an example. 

```{r}
library(tidyverse)
```

And we'll use our basketball log data. 

```{r}
logs <- read_csv("data/logs19.csv")
```

Here we're going to look at the scoring differential of teams. If you score more than your opponent, you win. So it stands to reason that if you score a lot more than your opponent over the course of a season, you should be very good, right? Let's see.

The first thing we're going to do is calculate that differential. Then, we'll group it by the team. After that, we're going to summarize using a new function called `cumsum` or cumulative sum -- the sum for each game as we go forward.

```{r}
difflogs <- logs %>% mutate(Differential = TeamScore - OpponentScore) %>% group_by(Team) %>% mutate(CumDiff = cumsum(Differential))
```

Now that we have the cumulative sum for each, let's filter it down to just Big Ten teams.

```{r}
bigdiff <- difflogs %>% filter(Conference == "Big Ten")
```

The step chart is it's own geom, so we can employ it just like we have the others. It works almost exactly the same as a line chart, but it just has to use the cumulative sum instead of a regular value.

```{r}
ggplot() + geom_step(data=bigdiff, aes(x=Date, y=CumDiff, group=Team))
```

Let's try a different element of the aesthetic: color, but this time inside the aesthetic. Last time, we did the color outside. When you put it inside, you pass it a column name and ggplot will color each line based on what thing that is, and it will create a legend that labels each line that thing. 

```{r}
ggplot() + geom_step(data=bigdiff, aes(x=Date, y=CumDiff, group=Team, color=Team))
```

From this, we can see two teams in the Big Ten have negative point differentials on the season -- Illinois and Rutgers. 

Let's look at those top teams plus Nebraska.

```{r}
nu <- bigdiff %>% filter(Team == "Nebraska Cornhuskers")
mi <- bigdiff %>% filter(Team == "Michigan Wolverines")
ms <- bigdiff %>% filter(Team == "Michigan State Spartans")
```

Let's introduce a couple of new things here. First, note when I take the color OUT of the aesthetic, the legend disappears. 

The second thing I'm going to add is the annotation layer. In this case, I am adding a text annotation layer, and I can specify where by adding in a x and a y value where I want to put it. This takes some finesse. After that, I'm going to add labels and a theme. 

```{r}
ggplot() + 
  geom_step(data=bigdiff, aes(x=Date, y=CumDiff, group=Team), color="light grey") +
  geom_step(data=nu, aes(x=Date, y=CumDiff, group=Team), color="red") + 
  geom_step(data=mi, aes(x=Date, y=CumDiff, group=Team), color="blue") + 
  geom_step(data=ms, aes(x=Date, y=CumDiff, group=Team), color="dark green") +
  annotate("text", x=(as.Date("2018-12-10")), y=220, label="Nebraska") +
  labs(x="Date", y="Cumulative Point Differential", title="Nebraska was among the league's most dominant", subtitle="Before the misseason skid, Nebraska was at the top of the Big Ten in point differential", caption="Source: Sports-Reference.com | By Matt Waite") +
  theme_minimal()
```

<!--chapter:end:16-stepcharts.Rmd-->

# Ridge charts

Ridgeplots are useful for when you want to show how different groupings compare with a large number of datapoints. So let's look at how we do this, and in the process, we learn about ggplot extensions. The extensions add functionality to ggplot, which doesn't out of the box have ridgeplots (sometimes called joyplots). 

In the console, run this: `install.packages("ggridges")`

Now we can add those libraries. 

```{r}
library(tidyverse)
library(ggridges)
```

So for this, let's look at every basketball game played since the 2014-15 season. That's more than 28,000 basketball games. [Download that data here](https://unl.box.com/s/u9407jj007fxtnu1vbkybdawaqg6j3fw).

```{r}
logs <- read_csv("data/logs1519.csv")
```

So I want to group teams by wins. Wins are the only thing that matter -- ask Tim Miles. So our data has a column called W_L that lists if the team won or lost. The problem is it doens't just say W or L. If the game went to overtime, it lists that. That complicates counting wins. And, with ridgeplots, I want to be be able to separate EVERY GAME by how many wins the team had over a SEASON. So I've got some work to do.

First, here's a trick to find a string of text and make that. It's called `grepl` and the basic syntax is grepl for this string in this field and then do what I tell you. In this case, we're going to create a new field called winloss look for W or L (and ignore any OT notation) and give wins a 1 and losses a 0. 


```{r}
winlosslogs <- logs %>% mutate(winloss = case_when(
  grepl("W", W_L) ~ 1, 
  grepl("L", W_L) ~ 0)
)
```

Now I'm going to add up all the winlosses for each team, which should give me the number of wins for each team. 

```{r}
winlosslogs %>% group_by(Team, Conference, season) %>% summarise(TeamWins = sum(winloss)) -> teamseasonwins
```

Now that I have season win totals, I can join that data back to my log data so each game has the total number of wins in each season. 

```{r}
winlosslogs %>% left_join(teamseasonwins, by=c("Team", "Conference", "season")) -> wintotallogs
```

Now I can use that same `case_when` logic to create some groupings. So I want to group teams together by how many wins they had over the season. For no good reason, I started with more than 25 wins, then did groups of 5 down to 10 wins. If you had fewer than 10 wins, God help your program. 

```{r}
wintotallogs %>% mutate(grouping = case_when(
  TeamWins > 25 ~ "More than 25 wins",
  TeamWins >= 20 & TeamWins <=25 ~ "20-25 wins",
  TeamWins >= 15 & TeamWins <=19 ~ "15-19 wins",
  TeamWins >= 10 & TeamWins <=14 ~ "10-14 wins",
  TeamWins < 10 ~ "Less than 10 wins")
) -> wintotalgroupinglogs
```

So my `wintotalgroupinglogs` table has each game, with a field that gives the total number of wins each team had that season and labeling each game with one of five groupings. I could use `dplyr` to do group_by on those five groups and find some things out about them, but ridgeplots do that visually.

Let's look at the differences in rebounds by those five groups. Do teams that win more than 25 games rebound better than teams that win fewer games? 

The answer might surprise you. 

```{r}
ggplot(wintotalgroupinglogs, aes(x = TeamTotalRebounds, y = grouping, fill = grouping)) +
  geom_density_ridges() +
  theme_ridges() + 
  theme(legend.position = "none")
```

Answer? Not really. Game to game, maybe. Over five seasons? The differences are indistiguishable. 

How about assists?

```{r}
ggplot(wintotalgroupinglogs, aes(x = TeamAssists, y = grouping, fill = grouping)) +
  geom_density_ridges() +
  theme_ridges() + 
  theme(legend.position = "none")
```

There's a little better, especially between top and bottom.

```{r}
ggplot(wintotalgroupinglogs, aes(x = Team3PPCT, y = grouping, fill = grouping)) +
  geom_density_ridges() +
  theme_ridges() + 
  theme(legend.position = "none")
```

If you've been paying attention this semester, you know what's coming next.

```{r}
ggplot(wintotalgroupinglogs, aes(x = TeamFGPCT, y = grouping, fill = grouping)) +
  geom_density_ridges() +
  theme_ridges() + 
  theme(legend.position = "none")
```

<!--chapter:end:17-ridgecharts.Rmd-->

# Lollipop charts

Second to my love of waffle charts because I'm always hungry, lollipop charts are an excellently named way of showing the difference between two things on a number line -- a start and a finish, for instance. Or the difference between two related things. Say, turnovers and assists. They aren't a geom, specifically, but you can assemble them out of points and segments, which are geoms. 

```{r}
library(tidyverse)
```

```{r}
logs <- read_csv("data/logs19.csv")
```

For the first example, let's look at the difference between a team's shooting performance and the conference's shooting performance as a whole. To get this, we're going to add up all the shots made by the conference, all the attempts taken by the conference, and then mutate a percentage based on that. 

```{r}
conferenceshooting <- logs %>% group_by(Conference) %>% summarise(totalshots = sum(TeamFG), totalattempts = sum(TeamFGA)) %>% mutate(conferenceshootingpct = totalshots/totalattempts)
```

Now I'm going to do the same with teams. 

```{r}
teamshooting <- logs %>% group_by(Team, Conference) %>% summarise(totalshots = sum(TeamFG), totalattempts = sum(TeamFGA)) %>% mutate(teamshootingpct = totalshots/totalattempts)
```

The last thing I need to do is join them together. So each team will have the conference shooting percentage as well as their own. 

```{r}
shooting <- teamshooting %>% left_join(conferenceshooting, by="Conference")
```

I have every team in college basketball, but that's insane. 

```{r}
big10 <- shooting %>% filter(Conference == "Big Ten")
```

So this takes a little doing, but the logic is pretty clear in the end. 

A lollipop chart is made up of two things -- a line between two points, and two points. So we need a geom_segment and two geom_points. And because they get layered starting at the bottom, our segment is first. A geom segment is made up of two things -- an x and a y value, and an x and y end. In this case, our x and xend are the same -- the Team -- and our y and yend are our two stats. For our points, both x values are the Team and the y is the different stats. What that does is put each point on the same line. 

```{r}
ggplot(big10) +
  geom_segment(aes(x=Team, xend=Team, y=teamshootingpct, yend=conferenceshootingpct), color="grey") + 
  geom_point(aes(x=Team, y=teamshootingpct), color="red") + 
  geom_point(aes(x=Team, y=conferenceshootingpct), color="blue") +
  coord_flip()
```

We can do better by changing the order of the teams by their shooting performance and giving it some theme love. 

```{r}
ggplot(big10) +
  geom_segment(aes(x=reorder(Team, teamshootingpct), xend=Team, y=teamshootingpct, yend=conferenceshootingpct), color="grey") + 
  geom_point(aes(x=reorder(Team, teamshootingpct), y=teamshootingpct), color="red") + 
  geom_point(aes(x=reorder(Team, teamshootingpct), y=conferenceshootingpct), color="blue") +
  coord_flip() +
   labs(x="", y="Shooting percentage vs league average", title="Except Purdue, shooting predicted Big Ten success", subtitle="The Boilermakers were average shooters, went deep in the NCAA tournament", caption="Source: sports-reference.com | By Matt Waite") +
  theme_minimal() + 
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 1),
    plot.subtitle = element_text(hjust = 1.3),
    axis.title = element_text(size = 10),
    axis.title.y = element_blank(),
    axis.text = element_text(size = 7),
    axis.ticks = element_blank()
  )
```

What if we wanted to order them by wins? Our data has a column called W_L that lists if the team won or lost. The problem is it doens't just say W or L. If the game went to overtime, it lists that. That complicates counting wins. Here's a trick to find a string of text and make that. It's called `grepl` and the basic syntax is grepl for this string in this field and then do what I tell you. In this case, we're going to create a new field called winloss look for W or L (and ignore any OT notation) and give wins a 1 and losses a 0.


```{r}
winlosslogs <- logs %>% mutate(winloss = case_when(
  grepl("W", W_L) ~ 1, 
  grepl("L", W_L) ~ 0)
)
```

So let's look at turnovers and assists. We'll call it give and take. Does the difference between those two things indicate something when we sort them by wins?

```{r}
giveandtake <- winlosslogs %>% group_by(Conference, Team) %>% summarise(turnovers = sum(TeamTurnovers), assists = sum(TeamAssists), wins=sum(winloss)) 
```

```{r}
big10gt <- giveandtake %>% filter(Conference == "Big Ten")
```


```{r}
ggplot(big10gt) +
  geom_segment(aes(x=reorder(Team, wins), xend=Team, y=turnovers, yend=assists), color="grey") + 
  geom_point(aes(x=reorder(Team, wins), y=turnovers), color="red") + 
  geom_point(aes(x=reorder(Team, wins), y=assists), color="blue") +
  coord_flip()
```

Short answer: Not really.




<!--chapter:end:18-lollipopcharts.Rmd-->

# Scatterplots

In several chapters of this book, we've been fixated on the Nebraska basketball team's shooting percentage, which took a nose dive during the season and ultimately doomed Tim Miles job. The question is ... does it matter?

This is what we're going to start to answer today. And we'll do it with scatterplots and correlations.

First, we need libraries and [data](https://unl.box.com/s/a8m91bro10t89watsyo13yjegb1fy009).

```{r}
library(tidyverse)
```

```{r}
logs <- read_csv("data/logs19.csv")
```

To do this, we need all teams and their season stats. How much, over the course of a season, does a thing matter? That's the question you're going to answer. 

In our case, we want to know how much does shooting percentage influence wins? How much different can we explain in wins with shooting percentage? We're going to total up the number of wins each team has and their season shooting percentage in one swoop.

Let's borrow from our ridgecharts work to get the correct wins and losses totals for each team. 

```{r}
winlosslogs <- logs %>% mutate(winloss = case_when(
  grepl("W", W_L) ~ 1, 
  grepl("L", W_L) ~ 0)
)
```

Now we can get a dataframe together that gives us the total wins for each team, and the total shots taken and made, which let's us calculate a season shooting percentage. 

```{r}
winlosslogs %>% 
  group_by(Team) %>%
  summarise(
    wins = sum(winloss),
    totalFGAttempts = sum(TeamFGA),
    totalFG = sum(TeamFG)
  ) %>%
  mutate(fgpct = totalFG/totalFGAttempts) -> fgmodel
```

Now let's look at the scatterplot. With a scatterplot, we put what predicts the thing on the X axis, and the thing being predicted on the Y axis. In this case, X is our shooting percentage, y is our wins.

```{r}
ggplot(fgmodel, aes(x=fgpct, y=wins)) + geom_point()
```

Let's talk about this. It seems that the data slopes up to the right. That would indicate a positive correlation between shooting percentage and wins. And that makes sense, no? You'd expect teams that shoot the ball well to win. But can we get a better sense of this? Yes, by adding another geom -- `geom_smooth`.

```{r}
ggplot(fgmodel, aes(x=fgpct, y=wins)) + geom_point() + geom_smooth(method=lm, se=TRUE)
```

But ... how strong a relationship is this? How much can shooting percentage explain wins? Can we put some numbers to this?

Of course we can. We can apply a linear model to this -- remember Chapter 8? We're going to create an object called fit, and then we're going to put into that object a linear model -- `lm` -- and the way to read this is "wins are predicted by field goal percentage". Then we just want the summary of that model.

```{r}
fit <- lm(wins ~ fgpct, data = fgmodel)
summary(fit)
```

Remember from Chapter 8: There's just a few things you really need.

The first thing: R-squared. In this case, the Adjusted R-squared value is .3462, which we can interpret as shooting percentage predicts about 35 percent of the variance in wins. Which sounds not great, but in social science, that's huge. That's great. A psychology major would murder for that R-squared.

Second: The P-value. We want anything less than .05. If it's above .05, the change between them is not statistically significant -- it's probably explained by random chance. In our case, we have 2.2e-16, which is to say 2.2 with 16 zeros in front of it, or .000000000000000022. Is that less than .05? Yes. Yes it is. So this is not random. Again, we would expect this, so it's a good logic test.

Third: The coefficient. In this case, the coefficient for fgpct is 149.416. Since I didn't convert percentages to decimals, what this says is that for every perentage point improvement in shooting percentage, we can expect the team to win 1.49 more games plus or minus some error. 

And we can use this to predict a team's wins: remember your algebra and y = mx + b. In this case, y is the wins, m is the coefficient, x is the shooting percentage and b is the intercept. 

So can plug these together: Expected wins = 149.416 * shooting percentage - 49.217

Let's use Nebraska as an example. They shot about .43 on the season (.4294421 to be exact). 

y = 149.416 * .4294421 - 49.217 or 14.95 wins. How many wins did Nebraska have? 19. 

What does that mean? It means that as disappointing a season as it was, Nebraska actually OVERPERFORMED it's season shooting percentage. They shouldn't have won as many games as they did. 

<!--chapter:end:19-scatterplots.Rmd-->

# Facet wraps

Sometimes the easiest way to spot a trend is to chart a bunch of small things side by side. Edward Tufte calls this "small multiples" where ggplot calls this a facet wrap or a facet grid, depending. 

One thing we noticed earlier in the semester -- it seems that a lot of teams shoot worse as the season goes on. Do they? We could answer this a number of ways, but the best way to show people would be visually. Let's use Small Mulitples.

As always, we start with libraries. 

```{r}
library(tidyverse)
```

Now data.

```{r}
logs <- read_csv("data/logs19.csv")
```

Let's narrow our pile and look just at the Big Ten.

```{r}
big10 <- logs %>% filter(Conference == "Big Ten")
```

The first thing we can do is look at a line chart, like we have done in previous chapters. 

```{r}
ggplot() + geom_line(data=big10, aes(x=Date, y=TeamFGPCT, group=Team)) + scale_y_continuous(limits = c(0, .7))
```

And, not surprisingly, we get a hairball. We could color certain lines, but that would limit us to focus on one team. What if we did all of them at once? We do that with a `facet_wrap`. The only thing we MUST pass into a `facet_wrap` is what thing we're going to separate them out by. In this case, we preceed that field with a tilde, so in our case we want the Team field. It looks like this: 

```{r}
ggplot() + geom_line(data=big10, aes(x=Date, y=TeamFGPCT, group=Team)) + scale_y_continuous(limits = c(0, .7)) + facet_wrap(~Team)
```

Answer: Not immediately clear, but we can look at this and analyze it. We could add a peice of annotation to help us out. 

```{r}
big10 %>% summarise(mean(TeamFGPCT))
```

```{r}
ggplot() + geom_hline(yintercept=.4447, color="dark grey") + geom_line(data=big10, aes(x=Date, y=TeamFGPCT, group=Team)) + scale_y_continuous(limits = c(0, .7)) + facet_wrap(~Team)
```

What do you see here? How do teams compare? How do they change over time? I'm not asking you these questions because they're an assignment -- I'm asking because that's exactly what this chart helps answer. Your brain will immediately start making those connections. 

## Facet grid vs facet wraps

Facet grids allow us to put teams on the same plane, versus just repeating them. And we can specify that plane as vertical or horizontal. For example, here's our chart from above, but using facet_grid to stack them.

```{r fig.height=20, fig.width=5}
ggplot() + geom_hline(yintercept=.4447, color="dark grey") + geom_line(data=big10, aes(x=Date, y=TeamFGPCT, group=Team)) + scale_y_continuous(limits = c(0, .7)) + facet_grid(Team ~ .)
```

And here they are next to each other:

```{r}
ggplot() + geom_hline(yintercept=.4447, color="dark grey") + geom_line(data=big10, aes(x=Date, y=TeamFGPCT, group=Team)) + scale_y_continuous(limits = c(0, .7)) + facet_grid(. ~ Team)
```

Note: We'd have some work to do with the labeling on this -- we'll get to that -- but you can see where this is valuable comparing a group of things. One warning: Don't go too crazy with this or it loses it's visual power.

## Other types

Line charts aren't the only things we can do. We can do any kind of chart in ggplot. Staying with shooting, where are team's winning and losing performances coming fromwhen we talk about team shooting and opponent shooting? 

```{r}
ggplot() + geom_point(data=big10, aes(x=TeamFGPCT, y=OpponentFGPCT, color=W_L)) + scale_y_continuous(limits = c(0, .7)) + scale_x_continuous(limits = c(0, .7)) + facet_wrap(~Team)
```

<!--chapter:end:20-facetwraps.Rmd-->

# Tables

But not a table. A table with features. 

Sometimes, the best way to show your data is with a table. R has a neat package called `formattable` and you'll install it like anything else with `install.packages('formattable')`. 

So what does it do? Let's gather our libraries and get some data. 

```{r}
library(tidyverse)
library(formattable)
```


```{r}
offense <- read_csv("data/offensechange.csv")
```


Let's ask this question: Which college football team saw the greatest improvement in yards per game this regular season? The simplest way to calculate that is by percent change. 

```{r}
changeTotalOffense <- offense %>%
  select(Name, Year, `Yards/G`) %>% 
  spread(Year, `Yards/G`) %>% 
  mutate(Change=(`2018`-`2017`)/`2017`) %>% 
  arrange(desc(Change)) %>% 
  top_n(20)
```
We've output tables to the screen a thousand times in this class with `head`, but formattable makes them look good with very little code. 

```{r}
formattable(changeTotalOffense)
```

So there you have it. Illinois improved the most. Because Nebraska gave them a quarterback, but I digress. First thing I don't like about formattable tables -- the right alignment. Let's fix that. 

```{r}
formattable(changeTotalOffense, align="l")
```

Next? I forgot to multiply by 100. No matter. Formattable can fix that for us. 

```{r}
formattable(
  changeTotalOffense, 
  align="l",
  list(
    `Change` = percent)
  )
```

Something else not great? I can't really see the magnitude of the 2018 column. A team could improve a lot, but still not gain that many yards (ahem, UTEP). Formattable has embeddable bar charts in the table. They look like this. 

```{r}
formattable(
  changeTotalOffense, 
  align="l",
  list(
    `2018` = color_bar("#FA614B"), 
    `Change` = percent)
  )
```
That gives me some more to mess with. 

One thing you can do is set the bar widths to the results of a function. In this case, it returns a number between 0 and 1, with 1 being the max and 0 being the minimum. It gives you some idea how far out UTEP is with their peers. 
```{r}
unit.scale = function(x) (x - min(x)) / (max(x) - min(x))

formattable(
  changeTotalOffense, 
  align="r",
  list(
    `2018` = color_bar("#FA614B", fun = unit.scale), 
    `2017` = color_bar("#FA614B", fun = unit.scale), 
    `Change` = percent)
  )
```

Another way to deal with this -- color tiles. Change the rectangle that houses the data to a color indicating the intensity of it. Again, UTEP stands out.

```{r}
formattable(
  changeTotalOffense, 
  align="r",
  list(
     area(col = 2:3) ~ color_tile("#FFF6F4", "#FA614B"),
    `Change` = percent)
  )
```

### Exporting tables

The first thing you need to do is install some libraries -- do this in the console, not in an R Studio code block because htmltools get's a little weird. 

```
install.packages("htmltools")
install.packages("webshot")

webshot::install_phantomjs()
```

Now, copy, paste and run this code block entirely. Don't change anything. 

```{r}
library("htmltools")
library("webshot")    

export_formattable <- function(f, file, width = "100%", height = NULL, 
                               background = "white", delay = 0.2)
    {
      w <- as.htmlwidget(f, width = width, height = height)
      path <- html_print(w, background = background, viewer = NULL)
      url <- paste0("file:///", gsub("\\\\", "/", normalizePath(path)))
      webshot(url,
              file = file,
              selector = ".formattable_widget",
              delay = delay)
    }
```

Now, save your formattable table to an object using the `<-` assignment operator. 

After you've done that, you can call the function you ran in the previous block to export as a png file. In my case, I created an object called table, which is populated with my formattable table. Then, in export_formattable, I pass in that `table` object and give it a name. 

```{r}
table <- formattable(
  changeTotalOffense, 
  align="r",
  list(
     area(col = 2:3) ~ color_tile("#FFF6F4", "#FA614B"),
    `Change` = percent)
  )

export_formattable(table,"table.png")
```

For now, pngs are what you need to export. There is a way to export PDFs, but they lose all the formatting when you do that, which is kind of pointless. 

<!--chapter:end:21-tables.Rmd-->

# Intro to rvest

All the way back in Chapter 2, we used Google Sheets and importHTML to get our own data out of a website. For me, that's a lot of pointing and clicking and copying and pasting. R has a library that can automate the harvesting of data from HTML on the internet. It's called `rvest`. 

Let's grab [a simple, basic HTML table from College Football Stats](http://www.cfbstats.com/2018/leader/national/team/offense/split01/category09/sort01.html). This is scoring offense for 2018. There's nothing particularly strange about this table -- it's simply formatted and easy to scrape. 

First we'll need some libraries. We're going to use a library called `rvest`, which you can get by running `install.packages('rvest')` in the console. 

```{r}
library(rvest)
library(tidyverse)
```

The rvest package has functions that make fetching, reading and parsing HTML simple. The first thing we need to do is specify a url that we're going to scrape.

```{r}
scoringoffenseurl <- "http://www.cfbstats.com/2018/leader/national/team/offense/split01/category09/sort01.html"
```

Now, the most difficult part of scraping data from any website is knowing what exact HTML tag you need to grab. In this case, we want a `<table>` tag that has all of our data table in it. But how do you tell R which one that is? Well, it's easy, once you know what to do. But it's not simple. So I've made a short video to show you how to find it. 

![](https://youtu.be/kYkSE3zWa9Y)

When you have simple tables, the code is very simple. You create a variable to receive the data, then pass it the url, read the html that was fetched, find the node you need using your XPath value you just copied and you tell rvest that it's a table. 

```{r}
scoringoffense <- scoringoffenseurl %>%
  read_html() %>%
  html_nodes(xpath = '//*[@id="content"]/div[2]/table') %>%
  html_table()
```

What we get from this is ... not a dataframe. It's a list with one element in it, which just so happens to be our dataframe. When you get this, the solution is simple: just overwrite the variable you created with the first list element.

```{r}
scoringoffense <- scoringoffense[[1]]
```

And what do we have? 

```{r}
head(scoringoffense)
```

We have data, ready for analysis. 

## A slightly more complicated example

What if we want more than one year in our dataframe?

This is a common problem. What if we want to look at every scoring offense going back several years? The website has them going back to 2009. How can we combine them? 

First, we should note, that the data does not have anything in it to indicate what year it comes from. So we're going to have to add that. And we're going to have to figure out a way to stack two dataframes on top of each other. 

So let's grab 2017.

```{r}
scoringoffenseurl17 <- "http://www.cfbstats.com/2017/leader/national/team/offense/split01/category09/sort01.html"

scoringoffense17 <- scoringoffenseurl17 %>%
  read_html() %>%
  html_nodes(xpath = '//*[@id="content"]/div[2]/table') %>%
  html_table()

scoringoffense17 <- scoringoffense17[[1]]
```

First, how are we going to know, in the data, which year our data is from? We can use mutate.

```{r error=TRUE}
scoringoffense18 <- scoringoffense %>% mutate(YEAR = 2018)
```

Uh oh. Error. What does it say? Column 1 must be named. If you look at our data in the environment tab in the upper right corner, you'll see that indeed, the first column has no name. It's the FBS rank of each team. So we can fix that and mutate in the same step. We'll do that using `rename` and since the field doesn't have a name to rename it, we'll use a position argument. We'll say rename column 1 as Rank. 

```{r}
scoringoffense18 <- scoringoffense %>% rename(Rank = 1) %>% mutate(YEAR = 2018)
scoringoffense17 <- scoringoffense17 %>% rename(Rank = 1) %>% mutate(YEAR = 2017)
```

And now, to combine the two tables together length-wise -- we need to make long data -- we'll use a base R function called `rbind`. The good thing is rbind is simple. The bad part is it can only do two tables at a time, so if you have more than that, you'll need to do it in steps.

```{r}
combined <- rbind(scoringoffense18, scoringoffense17)
```

Note in the environment tab we now have a data frame called combined that has 260 observations -- which just so happens to be what 130 from 2018 and 130 from 2017 add up to. 

```{r}
head(combined)
```

## An even more complicated example

What do you do when the table has non-standard headers? 

Unfortunately, non-standard means there's no one way to do it -- it's going to depend on the table and the headers. But here's one idea: Don't try to make it work. 

I'll explain.

Let's try to get [season team stats from Sports Reference](https://www.sports-reference.com/cbb/seasons/2019-school-stats.html). If you look at that page, you'll see the problem right away -- the headers span two rows, and they repeat. That's going to be all kinds of no good. 


First we'll grab the page.
```{r}
url <- "https://www.sports-reference.com/cbb/seasons/2019-school-stats.html"
```

Now, similar to our example above, we'll read the html, use XPath to find the table, and then read that table with a directive passed to it setting the header to FALSE. That tells rvest that there isn't a header row. Just import it as data. 

```{r}
stats <- url %>%
  read_html() %>%
  html_nodes(xpath = '//*[@id="basic_school_stats"]') %>%
  html_table(header=FALSE)
```

What we get back is a list of one element (similar to above). So let's pop it out into a data frame.

```{r}
stats <- stats[[1]]
```

And we'll take a look at what we have. 

```{r}
head(stats)
```

So, that's not ideal. We have headers and data mixed together, and our columns are named X1 to X34. Also note: They're all character fields. Because the headers are interspersed with data, it all gets called character data. So we've got to first rename each field. 

```{r}
stats <- stats %>% rename(Rank=X1, School=X2, Games=X3, OverallWins=X4, OverallLosses=X5, WinPct=X6, OverallSRS=X7, OverallSOS=X8, ConferenceWins=X9, ConferenceLosses=X10, HomeWins=X11, HomeLosses=X12, AwayWins=X13, AwayLosses=X14, ForPoints=X15, OppPoints=X16, Blank=X17, Minutes=X18, FieldGoalsMade=X19, FieldGoalsAttempted=X20, FieldGoalPCT=X21, ThreePointMade=X22, ThreePointAttempts=X23, ThreePointPct=X24, FreeThrowsMade=X25, FreeThrowsAttempted=X26, FreeThrowPCT=X27, OffensiveRebounds=X28, TotalRebounds=X29, Assists=X30, Steals=X31, Blocks=X32, Turnovers=X33, PersonalFouls=X34)
```

Now we have to get rid of those headers interspersed in the data. We can do that with filter that say keep all the stuff that isn't this.

```{r}
stats <- stats %>% filter(Rank != "Rk" & Games != "Overall") 
```

And finally, we need to change the file type of all the fields that need it. We're going to use a clever little trick, which goes like this: We're going to use `mutate_at`, which means mutate these fields. The pattern for `mutate_at` is `mutate_at` these variables and do this thing to them. But instead of specifying which of 33 variables we're going to mutate, we're going to specify the one we don't want to change, which is the name of the school. And we just want to convert them to numeric. Here's what it looks like:

```{r}
stats %>% mutate_at(vars(-School), as.numeric)
```

And just like that, we have a method for getting up to the minute season stats for every team in Division I. 

<!--chapter:end:22-rvest.Rmd-->

# Advanced rvest

With the chapter, we learned how to grab one table from one page. But what if you needed more than that? What if you needed hundreds of tables from hundreds of pages? What if you needed to combine one table on one page into a bigger table, but hundreds of times. There's a way to do this, it just takes patience, a lot of logic, a lot of debugging and, for me, a fair bit of swearing. 

So what we are after are game by game stats for each college basketball team in America. 

[We can see from this page](https://www.sports-reference.com/cbb/seasons/2019-school-stats.html) that each team is linked. If we follow each link, we get a ton of tables. But they aren't what we need. There's a link to gamelogs underneath the team names. 

So we can see from this that we've got some problems. 

1. The team name isn't in the table. Nor is the conference.
2. There's a date we'll have to deal with. 
3. Non-standard headers and a truly huge number of fields. 
4. And how do we get each one of those urls without having to copy them all into some horrible list? 

So let's start with that last question first and grab libraries we need.

```{r}
library(tidyverse)
library(rvest)
library(lubridate)
```

First things first, we need to grab the url to each team from that first link.

```{r}
url <- "https://www.sports-reference.com/cbb/seasons/2019-school-stats.html"
```

But notice first, we don't want to grab the table. The table doesn't help us. We need to grab the only *link* in the table. So we can do that by using the table xpath node, then grabbing the anchor tags in the table, then get only the link out of them (instead of the linked text). 

```{r}
schools <- url %>%
  read_html() %>%
  html_nodes(xpath = '//*[@id="basic_school_stats"]') %>%
  html_nodes("a") %>%
  html_attr('href')
```

Notice we now have a list called schools with ... 353 elements. That's the number of teams in college basketball, so we're off to a good start. Here's what the fourth element is. 

```{r}
schools[4]
```

So note, that's the relative path to Alabama A&M's team page. By relative path, I mean it doesn't have the root domain. So we need to add that to each request or we'll get no where. 

So that's a problem to note. 

Before we solve that, let's just make sure we can get one page and get what we need. 

We'll scrape Abilene Christian. 

To merge all this into one big table, we need to grab the team name and their conference and merge it into the table. But those values come from somewhere else. The scraping works just about the same, but instead of html_table you use html_text. 

So the first part of this is reading the html of the page so we don't do that for each element -- we just do it once so as to not overwhelm their servers. 

The second part is we're grabbing the team name based on it's location in the page. 

Third: The conference.

Fourth is the table itself, noting to ignore the headers. The last bit fixes the headers, removes the garbage header data from the table, converts the data to numbers, fixes the date and mutates a team and conference value. It looks like a lot, and it took a bit of twiddling to get it done, but it's no different from what you did for your last homework. 

```{r eval = FALSE}
page <- read_html("https://www.sports-reference.com/cbb/schools/abilene-christian/2019-gamelogs.html")
  
team <- page %>%
  html_nodes(xpath = '//*[@id="meta"]/div[2]/h1/span[2]') %>%
  html_text()

conference <- page %>%
    html_nodes(xpath = '//*[@id="meta"]/div[2]/p[1]/a') %>%
    html_text()

table <- page %>%
  html_nodes(xpath = '//*[@id="sgl-basic"]') %>%
  html_table(header=FALSE)

table <- table[[1]] %>% rename(Game=X1, Date=X2, HomeAway=X3, Opponent=X4, W_L=X5, TeamScore=X6, OpponentScore=X7, TeamFG=X8, TeamFGA=X9, TeamFGPCT=X10, Team3P=X11, Team3PA=X12, Team3PPCT=X13, TeamFT=X14, TeamFTA=X15, TeamFTPCT=X16, TeamOffRebounds=X17, TeamTotalRebounds=X18, TeamAssists=X19, TeamSteals=X20, TeamBlocks=X21, TeamTurnovers=X22, TeamPersonalFouls=X23, Blank=X24, OpponentFG=X25, OpponentFGA=X26, OpponentFGPCT=X27, Opponent3P=X28, Opponent3PA=X29, Opponent3PPCT=X30, OpponentFT=X31, OpponentFTA=X32, OpponentFTPCT=X33, OpponentOffRebounds=X34, OpponentTotalRebounds=X35, OpponentAssists=X36, OpponentSteals=X37, OpponentBlocks=X38, OpponentTurnovers=X39, OpponentPersonalFouls=X40) %>% filter(Game != "") %>% filter(Game != "G") %>% mutate(Team=team) %>% mutate_at(vars(-Team, -Date, -Opponent, -HomeAway, -W_L), as.numeric) %>% mutate(Date = ymd(Date)) %>% mutate(Team=team, Conference=conference)
```

Now what we're left with is how do we do this for ALL the teams. We need to send 353 requests to their servers to get each page. And each url is not the one we have -- we need to alter it. 

First we have to add the root domain to each request. And, each request needs to go to /2019-gamelogs.html instead of /2019.html. If you look at the urls two the page we have and the page we need, that's all that changes. 

What we're going to use is what is known in programming as a loop. We can loop through a list and have it do something to each element in the loop. And once it's done, we can move on to the next thing. 

Think of it like a program that will go though a list of your classmates and ask each one of them for their year in school. It will start at one end of the list and move through asking each one "What year in school are you?" and will move on after getting an answer. 

Except we want to take a url, add something to it, alter it, then request it and grab a bunch of data from it. Once we're done doing all that, we'll take all that info and cram it into a bigger dataset and then move on to the next one. Here's what that looks like:

```{r eval = FALSE}
uri <- "https://www.sports-reference.com"

logs <- tibble()

for (i in schools){
  log_url <- gsub("/2019.html","/2019-gamelogs.html", i)
  school_url <- paste(uri, log_url, sep="")  # creating the url to fetch
  
  page <- read_html(school_url)
  
  team <- page %>%
    html_nodes(xpath = '//*[@id="meta"]/div[2]/h1/span[2]') %>%
    html_text()
  
  conference <- page %>%
    html_nodes(xpath = '//*[@id="meta"]/div[2]/p[1]/a') %>%
    html_text()

  table <- page %>%
    html_nodes(xpath = '//*[@id="sgl-basic"]') %>%
    html_table(header=FALSE)

table <- table[[1]] %>% rename(Game=X1, Date=X2, HomeAway=X3, Opponent=X4, W_L=X5, TeamScore=X6, OpponentScore=X7, TeamFG=X8, TeamFGA=X9, TeamFGPCT=X10, Team3P=X11, Team3PA=X12, Team3PPCT=X13, TeamFT=X14, TeamFTA=X15, TeamFTPCT=X16, TeamOffRebounds=X17, TeamTotalRebounds=X18, TeamAssists=X19, TeamSteals=X20, TeamBlocks=X21, TeamTurnovers=X22, TeamPersonalFouls=X23, Blank=X24, OpponentFG=X25, OpponentFGA=X26, OpponentFGPCT=X27, Opponent3P=X28, Opponent3PA=X29, Opponent3PPCT=X30, OpponentFT=X31, OpponentFTA=X32, OpponentFTPCT=X33, OpponentOffRebounds=X34, OpponentTotalRebounds=X35, OpponentAssists=X36, OpponentSteals=X37, OpponentBlocks=X38, OpponentTurnovers=X39, OpponentPersonalFouls=X40) %>% filter(Game != "") %>% filter(Game != "G") %>% mutate(Team=team) %>% mutate_at(vars(-Team, -Date, -Opponent, -HomeAway, -W_L), as.numeric) %>% mutate(Date = ymd(Date)) %>% mutate(Team=team, Conference=conference)

  logs <- rbind(logs, table)  # binding them all together
  Sys.sleep(5)  # Sys.sleep(3) pauses the loop for 3s so as not to overwhelm website's server
}
```

The magic here is in `for (i in schools){`. What that says is for each iterator in schools -- for each school in schools -- do what follows each time. So we take the code we wrote for one school and use it for every school. 

This part:

```
  log_url <- gsub("/2019.html","/2019-gamelogs.html", i)
  school_url <- paste(uri, log_url, sep="")  # creating the url to fetch
  
  page <- read_html(school_url)
```

`log_url` is what changes our school page url to our logs url, and `school_url` is taking that log url and the root domain and merging them together to create the complete url. Then, page just reads that url we created. 

What follows that is taken straight from our example of just doing one.

The last bits are using rbind to take our data and mash it into a bigger table, over and over and over again until we have them all in a single table. Then, we tell our scraper to wait a few seconds because we don't want our script to machine gun requests at their server as fast as it can go. That's a guaranteed way to get them to block scrapers, and could knock them off the internet. Aggressive scrapers aren't cool. Don't do it. 

Lastly, we write it out to a csv file. 

```{r eval = FALSE}
write.csv(logs, "logs.csv")
```

So with a little programming knowhow, a little bit of problem solving and the stubbornness not to quit on it, you can get a whole lot of data scattered all over the place with not a lot of code. 

## One last bit

Most tables that Sports Reference sites have are in plain vanilla HTML. But some of them -- particularly player based stuff -- are hidden with a little trick. The site puts the data in a comment -- where a browser will ignore it -- and then uses javascript to interpret the commented data. To a human on the page, it looks the same. To a browswer or a scraper, it's invisible. You'll get errors. How do you get around it? 

1. Scrape the comments.
2. Turn the comment into text. 
3. Then read that text as html. 
4. Proceed as normal. 

```{r eval = FALSE}
h <- read_html('https://www.baseball-reference.com/leagues/MLB/2017-standard-pitching.shtml')

df <- h %>% html_nodes(xpath = '//comment()') %>%    # select comment nodes
    html_text() %>%    # extract comment text
    paste(collapse = '') %>%    # collapse to a single string
    read_html() %>%    # reparse to HTML
    html_node('table') %>%    # select the desired table
    html_table() 
```

<!--chapter:end:23-advancedrvest.Rmd-->

# Annotations

Some of the best sports data visualizations start with a provocative question. At a college just under three hours from Kansas City, my classes are lousy with Chiefs fans. So the first day of classes in the spring of 2019, I asked them: Are the Chief's Screwed in the Playoffs? The answer ultimately was yes, and how I was able to make that argument before a playoff game had even been played is a good example of how labeling and annotations can make a chart much better. 

Going to add a new library to the mix called `ggrepel`. You'll need to install it in the console with `install.packages("ggrepel")`. 

```{r}
library(tidyverse)
library(ggrepel)
```

Now we'll grab the data and join that data together using the Team name as the common element.

```{r}
offense <- read_csv("data/nfloffense.csv")

defense <- read_csv("data/nfldefense.csv")

total <- offense %>% left_join(defense, by="Team")

head(total)
```

I'm going to set up a point chart that places team on two-axes -- yards per play on offense on the x axis, and yards per play on defense. 

To build the annotations, I want the league average for offensive yards per play and defensive yards per play. We're going to use those as a proxy for quality. If your team averages more yards per play on offense, that's good. If they average fewer yards per play on defense, that too is good. So that sets up a situation where we have four corners, anchored by good at both and bad at both. The averages will create lines to divide those four corners up. 

```{r}
league_averages <- total %>% summarise(AvgOffYardsPer = mean(OffYardsPerPlay), AvgDefYardsPer = mean(DefYardPerPlay))

league_averages
```

I also want to highlight playoff teams and, of course, the Chiefs, since that was my question. Are they screwed. First, we filter them from our total list.

```{r}
playoff_teams <- c("Kansas City Chiefs", "New England Patriots", "Los Angeles Chargers", "Indianapolis Colts", "New Orleans Saints", "Los Angeles Rams", "Chicago Bears", "Dallas Cowboys", "Philadelphia Eagles")

playoffs <- total %>% filter(Team %in% playoff_teams)

chiefs <- total %>% filter(Team == "Kansas City Chiefs")
```

Now we create the plot. We have three geom_points, starting with everyone, then playoff teams, then the Chiefs. I alter the colors on each to separate them. Next, I add a geom_hline to add the horizontal line of my defensive average and a geom_vline for my offensive average. Next, I want to add some text annotations, labeling two corners of my chart (the other two, in my opinion, become obvious). Then, I want to label all the playoff teams. I use `geom_text_repel` to do that -- it's using the ggrepel library to push the text away from the dots, respective of other labels and other dots. It means you don't have to move them around so you can read them, or so they don't cover up the dots. 

The rest is just adding labels and messing with the theme. 

```{r}
ggplot() + 
  geom_point(data=total, aes(x=OffYardsPerPlay, y=DefYardPerPlay), color="light grey") +
  geom_point(data=playoffs, aes(x=OffYardsPerPlay, y=DefYardPerPlay)) +
  geom_point(data=chiefs, aes(x=OffYardsPerPlay, y=DefYardPerPlay), color="red") +
  geom_hline(yintercept=5.59375, color="dark grey") + 
  geom_vline(xintercept=5.590625, color="dark grey") + 
  geom_text(aes(x=6.2, y=5, label="Good Offense, Good Defense"), color="light blue") +
  geom_text(aes(x=5, y=6, label="Bad Defense, Bad Offense"), color="light blue") +
  geom_text_repel(data=playoffs, aes(x=OffYardsPerPlay, y=DefYardPerPlay, label=Team)) +
  labs(x="Offensive Yards Per Play", y="Defensive Points Per Play", title="Are the Chiefs screwed in the playoffs?", subtitle="Their offense is great. Their defense? Not so much", caption="Source: Sports-Reference.com | By Matt Waite") +
  theme_minimal() + 
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 10),
    axis.text = element_text(size = 7),
    axis.ticks = element_blank(),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  )
```

<!--chapter:end:24-annotations.Rmd-->

# Finishing touches, part 1

The output from ggplot is good, but not great. We need to add some pieces to it. The elements of a good graphic are:

* Headline
* Chatter
* The main body
* Annotations
* Labels
* Source line
* Credit line

That looks like:

```{r, echo=FALSE}
knitr::include_graphics(rep("images/chartannotated.png"))
```

## Graphics vs visual stories

While the elements above are nearly required in every chart, they aren't when you are making visual stories. 

* When you have a visual story, things like credit lines can become a byline.
* In visual stories, source lines are often a note at the end of the story. 
* Graphics don’t always get headlines – sometimes just labels, letting the visual story headline carry the load.

[An example from The Upshot](https://www.nytimes.com/interactive/2018/02/14/business/economy/inflation-prices.html). Note how the charts don't have headlines, source or credit lines.

## Getting ggplot closer to output

Let's explore fixing up ggplot's output before we send it to a finishing program like Adobe Illustrator. We'll need a graphic to work with first. 

```{r}
library(tidyverse)
```


```{r}
scoring <- read_csv("data/scoringoffense.csv")

total <- read_csv("data/totaloffense.csv")

offense <- total %>% left_join(scoring, by=c("Name", "Year"))

```

We're going to need this later, so let's grab Nebraska's 2018 stats from this dataframe. 

```{r}
nu <- offense %>% filter(Name == "Nebraska") %>% filter(Year == 2018)
```

We'll start with the basics.

```{r}
ggplot(offense, aes(x=`Yards/G`, y=`Points/G`)) + 
  geom_point(color="grey")
```

Let's take changing things one by one. The first thing we can do is change the figure size. Sometimes you don't want a square. We can use the `knitr` output settings in our chunk to do this easily in our notebooks. 

```{r fig.width=5, fig.height=2}

ggplot(offense, aes(x=`Yards/G`, y=`Points/G`)) + 
  geom_point(color="grey")

```

Now let's add a fit line. 

```{r fig.width=5, fig.height=2}

ggplot(offense, aes(x=`Yards/G`, y=`Points/G`)) + 
  geom_point(color="grey") + geom_smooth(method=lm, se=FALSE)

```

And now some labels.

```{r fig.width=5, fig.height=2}

ggplot(offense, aes(x=`Yards/G`, y=`Points/G`)) + 
  geom_point(color="grey") + geom_smooth(method=lm, se=FALSE) + 
  labs(x="Total yards per game", y="Points per game", title="Nebraska's underperforming offense", subtitle="The Husker's offense was the strength of the team. They underperformed.", caption="Source: NCAA | By Matt Waite")

```

Let's get rid of the default plot look and drop the grey background. 

```{r fig.width=5, fig.height=2}

ggplot(offense, aes(x=`Yards/G`, y=`Points/G`)) + 
  geom_point(color="grey") + geom_smooth(method=lm, se=FALSE) + 
  labs(x="Total yards per game", y="Points per game", title="Nebraska's underperforming offense", subtitle="The Husker's offense was the strength of the team. They underperformed.", caption="Source: NCAA | By Matt Waite") + 
  theme_minimal()
```

Off to a good start, but our text has no real heirarchy. We'd want our headline to stand out more. So let's change that. When it comes to changing text, the place to do that is in the theme element. [There are a lot of ways to modify the theme](http://ggplot2.tidyverse.org/reference/theme.html). We'll start easy. Let's make the headline bigger and bold.

```{r fig.width=5, fig.height=2}

ggplot(offense, aes(x=`Yards/G`, y=`Points/G`)) + 
  geom_point(color="grey") + geom_smooth(method=lm, se=FALSE) + 
  labs(x="Total yards per game", y="Points per game", title="Nebraska's underperforming offense", subtitle="The Husker's offense was the strength of the team. They underperformed.", caption="Source: NCAA | By Matt Waite") + 
  theme_minimal() + 
  theme(
    plot.title = element_text(size = 16, face = "bold")
    ) 
```

Now let's fix a few other things -- like the axis labels being too big, the subtitle could be bigger and lets drop some grid lines.

```{r fig.width=5, fig.height=2}

ggplot(offense, aes(x=`Yards/G`, y=`Points/G`)) + 
  geom_point(color="grey") + geom_smooth(method=lm, se=FALSE) + 
  labs(x="Total yards per game", y="Points per game", title="Nebraska's underperforming offense", subtitle="The Husker's offense was the strength of the team. They underperformed.", caption="Source: NCAA | By Matt Waite") + 
  theme_minimal() + 
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 8), 
    plot.subtitle = element_text(size=10), 
    panel.grid.minor = element_blank()
    ) 
```

Missing from this graph is the context that the headline promises. Where is Nebraska? We haven't added it yet. So let's add a point and a label for it. 

```{r fig.width=5, fig.height=2}

ggplot(offense, aes(x=`Yards/G`, y=`Points/G`)) + 
  geom_point(color="grey") + geom_smooth(method=lm, se=FALSE) + 
  labs(x="Total yards per game", y="Points per game", title="Nebraska's underperforming offense", subtitle="The Husker's offense was the strength of the team. They underperformed.", caption="Source: NCAA | By Matt Waite") + 
  theme_minimal() + 
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 8), 
    plot.subtitle = element_text(size=10), 
    panel.grid.minor = element_blank()
    ) +
  geom_point(data=nu, aes(x=`Yards/G`, y=`Points/G`), color="red") + 
  geom_text_repel(data=nu, aes(x=`Yards/G`, y=`Points/G`, label="Nebraska 2018"))
```

If we're happy with this output -- if it meets all of our needs for publication -- then we can simply export it as a png file. We do that by adding `+ ggsave("plot.png", width=5, height=2)` to the end of our code. Note the width and the height are from our knitr parameters at the top -- you have to repeat them or the graph will export at the default 7x7. 

If there's more work you want to do with this graph that isn't easy or possible in R but is in Illustrator, simply change the file extension to `pdf` instead of `png`. The pdf will open as a vector file in Illustrator with everything being fully editable. 

<!--chapter:end:25-finishingtouches1.Rmd-->

# Finishing Touches 2

Frequently in my classes, students use the waffle charts library quite extensively to make graphics. This is a quick walkthough on how to get a waffle chart into a publication ready state. 

```{r}
library(waffle)
```

Let's look at the offensive numbers from Nebraska v. Wisconsin football game. Nebraska lost 41-24, but Wisconsin gained only 15 yards more than Nebraska did. You can find the [official stats on the NCAA's website](https://www.ncaa.com/game/football/fbs/2018/10/06/nebraska-wisconsin/team-stats).

I'm going to make two vectors for each team and record rushing yards and passing yards. 

```{r}
nu <- c("Rushing"=111, "Passing"=407, 15)
wi <- c("Rushing"=370, "Passing"=163, 0)
```

So what does the breakdown of Nebraska's night look like? How balanced was the offense? 

The waffle library can break this down in a way that's easier on the eyes than a pie chart. We call the library, add the data, specify the number of rows, give it a title and an x value label, and to clean up a quirk of the library, we've got to specify colors. 

**ADDITIONALLY**

We can add labels and themes, but you have to be careful. The waffle library is applying it's own theme, but if we override something they are using in their theme, some things that are hidden come back and make it worse. So here is an example of how to use ggplot's `labs` and the theme to make a fully publication ready graphic. 

```{r}
waffle(nu/10, rows = 5, xlab="1 square = 10 yards", colors = c("black", "red", "white")) + labs(title="Nebraska vs Wisconsin on offense", subtitle="The Huskers couldn't get much of a running game going.", caption="Source: NCAA | Graphic by Matt Waite") + 
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 10),
    axis.title.y = element_blank()
  )
```

Note: The alignment of text sucks. 

How to fix that? We can use ggsave to a pdf and fix it in Illustrator.

```{r eval = FALSE}
waffle(nu/10, rows = 5, xlab="1 square = 10 yards", colors = c("black", "red")) + labs(title="Nebraska vs Wisconsin on offense", subtitle="The Huskers couldn't get much of a running game going.", caption="Source: NCAA | Graphic by Matt Waite") + 
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 10),
    axis.title.y = element_blank()
  ) + ggsave("waffle.pdf")
```

But what if we're using a waffle iron? And what if we want to change the output size? It gets tougher. 

Truth is, I'm not sure what is going on with the sizing. You can try it and you'll find that the outputs are ... unpredictable. 

Things you need to know about waffle irons:

* They're a convenience method, but all they're really doing is executing two waffle charts together. If you don't apply the theme to both waffle charts, it breaks.
* You will have to get creative about applying headline and subtitle to the top waffle chart and the caption to the bottom. 
* Using ggsave doesn't work either. So you'll have to use R's pdf output. 

Here is a full example. I start with my waffle iron code, but note that each waffle is pretty much a self contained thing. That's because a waffle iron isn't really a thing. It's just a way to group waffles together, so you have to make each waffle individually. My first waffle has the title and subtitle but no x axis labels and the bottom one has not title or subtitle but the axis labels and the caption.  

```{r}
iron(
 waffle(
   nu/10, 
   rows = 2, 
   colors = c("black", "red", "white")) + 
   labs(title="Nebraska vs Wisconsin: By the numbers", subtitle="The Huskers couldn't run, Wisconsin could.") + 
   theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 10),
    axis.title.y = element_blank()
  ),
 waffle(
   wi/10, 
   rows = 2, 
   xlab="1 square = 10 yards", 
   colors = c("black", "red", "white")) + labs(caption="Source: NCAA | Graphic by Matt Waite")
) 
```

If you try to use ggsave on that, you'll only get the last waffle chart. Like I said, irons aren't really anything, so ggplot ignores them. So to do this, we have to use R's pdf capability. 

Here's the same code, but wrapped in the R pdf functions. The first line says we're going to output this as a pdf with this name. Then my code, then `dev.off` to tell R that's what I want as a PDF. Don't forget that. 

```{r eval=FALSE}
pdf("waffleiron.pdf")
iron(
 waffle(
   nu/10, 
   rows = 2, 
   colors = c("black", "red", "white")) + 
   labs(title="Nebraska vs Wisconsin: By the numbers", subtitle="The Huskers couldn't run, Wisconsin could.") + 
   theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 10),
    axis.title.y = element_blank()
  ),
 waffle(
   wi/10, 
   rows = 2, 
   xlab="1 square = 10 yards", 
   colors = c("black", "red", "white")) + labs(caption="Source: NCAA | Graphic by Matt Waite")
) 
dev.off()
```


It probably still needs work in Illustrator, but less than before. 


<!--chapter:end:26-finishingtouches2.Rmd-->

# Assignments

This is a collection of assignments I've used in my Sports Data Analysis and Visualization course at the University of Nebraska-Lincoln. The overriding philosophy is to have students do lots of small assignments that directly apply what they learned, and often pulling from other assignments. Each small assignment is just a few points each -- I make them 5 points each and make the grading a yes/no decision on 5 different questions -- so a bad grade on one doesn't matter. Then, twice during the semester, I have them create blog posts with visualizations on a topic of their choosing. The topic must have a point of view -- Nebraska's woes on third down are why the team is struggling, for example -- and must be backed up with data. They have to write a completely documented R Notebook explaining what they did and why; they have to write a publicly facing blog post for a general audience and that post has to have at least three graphs backing up their point; and they have to give a lightning talk (no more than five minutes) in class about what they have found. Those two assignments are typically worth 50 percent of the course grade. 


**Chapter 1**

Create an R notebook (which you should have done if you were following along). In it, delete all the generated text from it, so you have a blank document. Then, write a sentence in the document telling me what the last thing you did in Swirl was. Then add a code block (see above about inserting) and add two numbers together. Any two numbers. Run that code block. Save the file and submit the .Rmd file created when you save it. 

That's it. Simple.

**Chapter 2**

**Chapter 3**

We're going to put it all together now. We're going to calculate the mean and median salaries of departments at each campus using the salary data. 

Answer these questions:

1. What are the mean and median salaries of each department on each campus? And how many employees are on each department?
2. What are the median salaries of the largest departments at each campus? And how does that compare to the average salary for that department?
3. What department has the highest mean salary? How does that compare with the median?

To do this, you'll need to [download the salary data data](https://www.dropbox.com/s/xts9xsim3lpg7qu/nusalaries1819.csv?dl=0).

Rubric

1. Did you read the data into a dataframe? 
2. Did you use group by syntax correctly? 
3. Did you use summarize syntax correctly?
4. Did you use Markdown comments to explain your steps? 


**Chapter 4**

Read the Wall Street Journal story about declining attendance -- in Files in Canvas called WSJEmptySeats.pdf. Look at the walkthrough and be ready to talk about similarities and differences on Monday. 

Calculate the percent change in an rushing offense category of your choosing [in this dataset](https://www.dropbox.com/s/bxqmzntkwhqn24e/rushingoffense.csv?dl=0). Do not pick Games. Look at something else. Who gained the most in your category? Who lost the most? Explain your steps, and write a paragraph on what you found. 

#### Rubric

1. Did you import the data correctly?
2. Did you mutate the data correctly? Did you do it in one step?
3. Did you sort the data correctly?
4. Did you explain each step using Markdown?


**Chapter 5**

### Assignment

Let's have some more fun with the [salary database](https://www.dropbox.com/s/xts9xsim3lpg7qu/nusalaries1819.csv?dl=0) is out! Time to use filters, selects and top_n.

For the first question, [use this dataset](https://www.dropbox.com/s/iodsuouf39blbbz/coaches.csv?dl=0) of just coach salaries. The second two questions use the salaries database you already used.

1. What is the average and median salaries for coaches at UNL, UNK and UNO?
2. Who are the top 10 highest paid employees at UNL (using top_n)?
3. What is my salary?

### If you're curious how I got the coaches salaries separated out

First, we import the data. 

```{r eval=FALSE}
salaries <- read_csv("../../Data/nusalaries1819.csv")
```

To filter out positions with the word Coach in it, we're going to have to use a new library that comes with the tidyverse, meaning you have it installed already. It's called StringR and it works with text. 

```{r eval=FALSE}
library(stringr)
```

We're going to use a function in StringR called str_detect, which does what you think it does -- it detects if a set of characters are in a piece of data. So in this case, we want to find all the coaches, so we're going to look for the word Coach. But, when I do that, I get some people who have coach in their title but aren't coaches. 

```{r eval=FALSE}
salaries %>% filter(str_detect(Position, 'Coach'))
```
I don't know about you, but's been a while since I watched a team headed by something called an Early Childhood Coach. Or a Career Coach. So, we need to filter them out. But how do we know which ones to filter? Let's use group by to get us a list of departments. That will be the giveaway -- there aren't sports coaches in the College of Arts and Sciences.

```{r eval=FALSE}
salaries %>% filter(str_detect(Position, 'Coach')) %>% group_by(Department) %>% summarize(total=n())
```

From that, we can build a list of the non sports related departments. 

```{r eval=FALSE}
noncoach = c("Academic & Career Development Center", "College of Arts & Sciences", "College of Business", "First Year Exp & Transition Programs", "NE Ctr  Rsrch on Youth,Fam & School", "Program in English Second Language")
```

And we can filter them out by putting a ! in front of the logical statement, which turns it into a negative. So here, we have Department NOT IN our list of non sports departments. 

```{r eval=FALSE}
salaries %>% filter(str_detect(Position, 'Coach')) %>% filter(!Department %in% noncoach)
```

Let's do ourselves a favor and create a new data frame from this and use that instead:

```{r eval=FALSE}
coaches <- salaries %>% filter(str_detect(Position, 'Coach')) %>% filter(!Department %in% noncoach)
```


**Chapter 6**

### Assignment

* Read [A Layered Grammar of Graphics](https://byrneslab.net/classes/biol607/readings/wickham_layered-grammar.pdf) by Hadley Wickham. 
* Watch Wickham, who created the libraries we are using, [work through a data project](https://www.youtube.com/watch?v=go5Au01Jrvs). 

**Chapter 7**

**Chapter 8**

### Assignment

The fun of this is that you can test all kinds of theories. Given what we have in our dataset, what do you think predicts Nebraska's win total? Rebounds? Rebound differential? Fouls? Three point shooting? Total up every team's season performance, produce a scatterplot and a linear model and evaluate. Does it? What's your R-squared? What's your P-value? Create a model -- y = mx + b -- and see how Nebraska is performing relative to expectations. 

##### Rubric

1. Did you import the data correctly?
2. Did you summarize it correctly?
3. Did you produce a scatterplot?
4. Did you produce a linear model?
5. Did you apply the model?
6. Did you interpret your results?
7. Did you comment your code in markdown?


**Chapter 9**

#### Assignment 

You have been hired by Fred Hoiberg to build a team. He's interested in this model, but wants more.

There are more predictors to be added to our model. You are to find two. Two that contribute to the predictive quality of the model without largely overlapping another predictor.

In your notebook, report the adjusted r-squared you achieved. 

You are to generate a new set of coefficients, a new formula and a new set of numbers of what a conference champion would expect in terms of differential. I've done a lot of work for you. Continue it. Add two more predictors and complete the prediction. And compare that to Nebraska of this season.

Turn in your notebook with these answers and comments to the code you added, making sure to add WHY you are doing things. Why did you select those two variables.






**Chapter 10**

**Chapter 11**

## Assignment

Refine the composite Z Score I started here. Add two more elements to it. What else do you think is important to the success of a basketball team? I've got shooting, rebounds and the opponents shooting. What else would you add? 

In an R Notebook, make your case for the two elements you are adding. Then, follow my steps here until you get to the `teamquality` dataframe step, where you'll need to add the fields you are going to add to the composite. Then you'll need to add your fields to the `teamtotals` dataframe. Then you'll need to adjust `teamzscore`.  

Finally, look at your ranking of Big Ten teams and compare it to mine. Did adding more elements to the composite change anything? Explain the differences in your notebook. Which one do you think is more accurate? I won't be offended if you say yours, but why do you feel that way? 

[The data you'll need is here](https://www.dropbox.com/s/k6s758v5gwy7n4z/logs.csv?dl=0). 

#### Rubric

1. Did you import the data correctly?
2. Did you mutate the data correctly? 
3. Did you sort the data correctly?
4. Did you explain each step using Markdown?



**Chapter 12**

### Assignment

[Take this same attendance data](https://www.dropbox.com/s/m52dkdon3zs3ssq/attendance.csv?dl=0). I want you to produce a bar chart of the top 10 schools by percent change in attendance between 2017-2018 and 2013-2014. I want you to change the title and the labels and I want you to apply a theme different from the ones I used above. You can find [more themes in the ggplot documentation](https://ggplot2.tidyverse.org/reference/ggtheme.html).

#### Rubric
1. Did you import the data correctly?
2. Did you manipulate the data correctly?
3. Did you chart the data?
4. Did you explain your steps in Markdown comments?

**Chapter 13**

### Assignment

I want you to make this same chart, except I want you to make the weight the percentage of the total number of graduates that gender represents. You'll be mutating a new field to create that percentage. You'll then chart it with the fill. The end result should be a stacked bar chart allowing you to compare genders between universities. 

#### Rubric

1. Did you import the data correctly?
2. Did you mutate the data correctly?
3. Did you save that new data to a new data frame?
4. Did you chart it correctly with fill?


**Chapter 14**

### Assignment

Compare Nebraska and Michgan using a Waffle chart and another metric than what I've done above for the game last night.

[Here's the library's documentation](https://github.com/hrbrmstr/waffle).
[Here's the stats from the game](https://www.sports-reference.com/cbb/boxscores/2019-02-28-19-michigan.html).

Turn in your notebook with your waffle chart. It must contain these two things:

* Your waffle chart
* A written narrative of what it says. What does your waffle chart say about how that game turned out?

**Chapter 15**

### Assignment

* How does Nebraska's shooting percentage compare to the Big Ten? Put the Big Ten on the same chart as Nebraska, you'll need two dataframes, two geoms and with your Big Ten dataframe, you need to use `group` in the aesthetic. 

* After working on this chart, your boss comes in and says they don't care about field goal percentage anymore. They just care about three-point shooting because they read on some blog that three-point shooting was all the rage. Change what you need to change to make your line chart now about how the season has gone behind the three-point line. How does Nebraska compare to the rest of the Big Ten?

#### Rubric

1. Did you gather the data correctly?
2. Did you import it into R Notebook correctly?
3. Did you create the data frames needed to chart correctly?
4. Did you chart both correctly?

**Chapter 16**


### Assignment

Re-make this, but with rebounding. I want you to visualize the differential between our rebounds and their rebounds, and then plot the step chart showing over the course of the season. Highlight Nebraska. Highlight the top team. Add annotation layers to label both of them. 

#### Rubric

1. Did you import the data correctly?
2. Did you mutate the data correctly? Did you do it in one step?
3. Did you chart the data correctly?
4. Did you annotate your data?
5. Did you explain each step using Markdown?


**Chapter 17**

#### Assignment

You've been hired by Fred Hoiberg to tell him how to win the Big Ten. He's not impressed with that I came up with. So what you need to do is look for a *composite* measure that produces a meaningful ridgeplot. What that means is you're going to mutate `wintotalgroupinglogs` one more time. Is the differential between rebounding meaningful instead of just the total? Or assists? Or something else? Your call. Your goal is to produce a ridgeplot that tells The Mayor he needs to focus on doing X better than the opponent to win a Big Ten title.

##### Rubric

1. Did you mutate the data correctly?
2. Did you produce a ridgeplot?
3. Does it show a difference between top teams and the rest?


**Chapter 18**


#### Assignment

You've been hired by Fred Hoiberg to tell him how to win the Big Ten. He's not impressed with that I came up with. So what else could you look at with lollipop charts? Your call. Your goal is to produce a lollipop chart that tells The Mayor he needs to focus on the gap between X and Y if he wants to win a Big Ten title.

##### Rubric

1. Did you manipulate the data correctly?
2. Did you produce a lollipop chart?
3. Does it show a difference?


**Chapter 19**


### Assignment

The fun of this is that you can test all kinds of theories. Given what we have in our dataset, what do you think predicts Nebraska's win total? Rebounds? Rebound differential? Fouls? Three point shooting? Total up every team's season performance, produce a scatterplot and a linear model and evaluate. Does it? What's your R-squared? What's your P-value? Create a model -- y = mx + b -- and see how Nebraska is performing relative to expectations. 

##### Rubric

1. Did you import the data correctly?
2. Did you summarize it correctly?
3. Did you produce a scatterplot?
4. Did you produce a linear model?
5. Did you apply the model?
6. Did you interpret your results?
7. Did you comment your code in markdown?


**Chapter 20**

#### Assignment

How else would you look at teams going into the Big 10 Tournament using facet wraps? In class, create one using the log data we've used (if you want the updated version, [download it again](https://www.dropbox.com/s/0lpvstsjziz5k6p/logs.csv?dl=0)). You can just lightly comment your code. I'm more curious for you to try it. 




**Chapter 21**


### Assignment

We now have [season logs](https://www.dropbox.com/s/0lpvstsjziz5k6p/logs.csv?dl=0) for every college basketball game played up to the NCAA tournament.

Create a dataframe that shows the 10 best or 10 worst at something. Or rank the Big Ten. Your choice. Then use formattable to show in both a table and visually how the best were different from the worst. 

Export it to a PNG using the example above. Then, in Illustrator, add a headline, chatter, source and credit lines. Turn in the PNG file you export from Illustrator.


**Chapter 22**

#### Assignment

I am a huge Premiere League fan, so I want data on the league. [For now, I just want teams](https://fbref.com/en/comps/9/stats/Premier-League-Stats). Scrape the team data at the top, but before you do, look at the header. Is it one row? Does that make it standard? Nope. So what now? 

##### Rubric

1. Did you scrape the page?
2. Did you finish with a clean dataset?


**Chapter 23**

**Chapter 24**

**Chapter 25**

**Chapter 26**

**Chapter 27**

<!--chapter:end:27-assignments.Rmd-->

